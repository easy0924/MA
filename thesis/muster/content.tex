%% content.tex
%%

%% ===========================
\chapter{Foundations}
\label{ch:Foundations}
%% ===========================

This chapter provides an introduction to fundamental knowledge that are relevant to this work. We start with the whole SMT system and pre-reordering system first, then followed by the information we used to create the reordering rules including alignment, POS tag, syntax tree. At the end we shows the different rule types, the oracle reordering and how to build lattices for translation. \cite{book} also provide a good introduction to statistical machine translation, including different kinds of theories and methods that are relevant to this work.

%% ===========================
\section{SMT System}
\label{ch:Foundations:sec:SMTSystem}
%% ===========================

%Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.\cite{wikismt} One typical archit

Statistical machine translation (SMT) is the state of art machine translation paradigm. It uses a typical log-linear model which is composed of a decoder and different statistical models including phrase table, reordering model and language model. All the models are weighted with parameters which are tuned from the development data. Besides development data, training data are used for training the alignment, phrase table and other models. And test data are used for evaluation purpose. The architecture of a SMT system could be illustrated as figure $2.1$.

\begin{figure}[H]
\centering
\input{smt.tikz}
\caption{Architecture of SMT system}
\end{figure}

%% ===========================
\section{Rules Based Pre-Reodering}
\label{ch:Foundations:sec:PreReorderingSystem}
%% ===========================

Our pre-reordering method is based on reordering rules. Reordering rules are rules that tell us how we should reordering the sentences in source language before translating them. In our system, the rules are generated by using the word alignment, part-of-speech (POS) tag and syntax tree, all of which are calculated based on the training data. After we apply the reordering rules to the source sentences, word lattices are generated. The word lattices contains all the reordering possibilities of the source sentences and are further passed to the decoder for translating. The pre-reodering system could be illustrated as figure $2.2$.

\begin{figure}[H]
\centering
\input{prereordering.tikz}
\caption{Pre-reordering system}
\end{figure}

A more detailed description of \hyperref[ch:Foundations:sec:Alignment]{word alignment}, \hyperref[ch:Foundations:sec:PosTag]{POS tag}, \hyperref[ch:Foundations:sec:SyntacticTree]{syntax tree},
\hyperref[ch:Foundations:sec:types]{reordering rules} and 
\hyperref[ch:Foundations:sec:Lattices]{word lattices} is also clarified in the following sections.

The \hyperref[ch:ReorderingApproach]{reordering approach} we used for extracting and applying the rules is introduced in the next chapter.

%% ===========================
\section{Word Alignment}
\label{ch:Foundations:sec:Alignment}
%% ===========================

Word alignment indicate the possible alignment between words in the source sentence and words in the target sentence. For example, figure $2.3$ shows an alignment between an English sentence and a Chinese sentence.

\begin{figure}[H]
\centering
\input{alignment.tikz}
\caption{Example of word alignment}
\end{figure}
%? better example with crossing alignment?
Or it may be simply presented as index pairs.
\begin{center}
\verb|7-1 8-1 8-2 9-3 9-4 1-5 1-6 2-6 5-6 3-7 4-8 4-9 10-10|
\end{center}

In the figure, the words that are aligned through lines in two languages have related meaning. We can see how the words are reordered from the figure. For exmaple, the noun clause ``Clinton's presidency'' is completely moved forward to the front of the sentence.

\phantomsection\label{alignedrange}
It is noteworthy that the alignment is not always a one-to-one match. In the example, the word ``of'' is not aligned at all and the word ``months'' is aligned to two word ``\cntext{个}'' and ``\cntext{月}'' in Chinese. We can define the aligned range as the range from the first word a certain word is aligned to to the last word it is aligned to. For example, the aligned range of word ``there'' is $[5,6]$. The \textbf{aligned range} can be coincided, such as the $6$th word ``\cntext{剩下}'' in Chinese is also aligned to ``are'' and ``left'' at the same time besides ``there''. The coincidence sometimes makes the detection of rules more difficult, because the word order is not clear any more.

The word alignment could be trained with the GIZA++ tool by using Expectation Maximization (EM) algorithm. From the word alignment of training data we can see the patterns how the words are reordered before and after translation. Therefore, we could extract these reordering rules and apply them on the text, which is to be translated.

%% ===========================
\section{Part-of-Speech Tag}

Part-of-speech (POS) tags are markups of words in the text, which corresponds their linguistic role in the text. Depends on the definition of the roles, the set of POS tags could be different. Besides, different languages may have different POS tag set, since they may have different linguistic features, which are relevant to translation.
\begin{figure}[H]
\centering
\input{tags.tikz}
\caption{Example of POS tags}
\end{figure}
Figure $2.4$ shows an tagged English sentence. 

Tagset? English \& Chinese how to tag?


\label{ch:Foundations:sec:PosTag}
%% ===========================

%% ===========================
\section{syntax tree}
\label{ch:Foundations:sec:SyntacticTree}
%% ===========================

The syntax tree shows the syntactic structure of a sentence and can be very useful for word reordering. A syntax tree contains two kinds of nodes: the leaves and the internal nodes. Each leaf presents a word in the sentence, and is annotated with the POS tag. And each internal node presents a constituents, which is also annotated to indicate its category or syntactic role. In the Penn treebank \citep{penn}, for example, the annotation ``NP'' means noun phrase and the annotation ``S'' means simple declarative clause. Figure $2.5$ is an example of a syntax tree.

\begin{figure}[H]
\centering
\input{ParseTree.tikz}
\caption{Example of a parse tree}
\end{figure}

We can see the syntactic structure of the sentence from the syntax tree very clear. In this example, The words ``math and biology exams'' make up a noun clause, which plays the roll of subject. The predicate has a nested structure of verb clause, because it's compound with the modal verb ``will'' and the word ``be''. And ``on the 27th'' is a preposition clause in the verb clause, which is again composed of a preposition ``in'' and a noun clause ``the 27th''.

%%% ===========================
%\section{Dependency Tree}
%\label{ch:Foundations:sec:DependencyTree}
%%% ===========================

%% ===========================
\section{Reordering Rules}
\label{ch:Foundations:sec:types}
%% ===========================
%Short rules was introduced by \cite{short}.
%\subsection{Short Rules}
%\subsection{Long Rules}
%\subsection{Tree Rules}

Based on \cite{short}, \cite{long}, \cite{tree} and \cite{combine}, we introduce the different rule types, rule combination, how to decide if rules can be extracted as well as how to calculate the probability to apply the rules.

\subsection{Short Rules}

Short rules are extracted based on the sequences of adjacent words or their POS tags in the sentence to translate. Sequences of adjacent words or tags are observed, rules are then extracted if the same reordering patterns appear frequently. Following are some examples:
$$\verb|after the accident -> the accident after (0.5)|$$
$$\verb|WRB MD DT -> DT WRB DT (0.3)|$$
The first rule in this example shows, if a the word sequence ``after the accident'' ever appears in the text, it should be reordered to ``the accident after'' with a probability $0.5$, so will the word order be more consistent with the translation. The second rule shows, if the word sequence of a wh-adverb (when, where, why, etc.), a modal verb (MD) and a determiner (DT) appears, the determiner should be moved before the wh-adverb with a probablity of $0.3$.

In addition, Short rules also have some different varieties \citep{short}:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Tag sequence:} rules are extracted based on adjacent tag sequence
\item \textbf{Word sequence:} rules are extracted based on adjacent word sequence
\item \textbf{Context of one or two tags before and/or after the tag sequence}
\item \textbf{Context of one or two words before and/or after the tag sequence}
\end{itemize}

\subsection{Long Rules}

Long rules are specially designed to help the long distance word reordering for translation between English and German. The rules are based on POS tags of the text, and an example is as following:
$$\verb|NN X MD VHP -> X MD NN VHP (0.14)|$$

The ``X'' in the example is an placeholder, which can replace one or more words. ``VHP'' is the right context, which is sometimes helpful to define the reordering boundary. ``NN'' means noun, ``MD'' means modal verb and ``VHP'' is the word ``have''. In this example, the tag sequence ``NN X MD VHP'' will be reordered as ``X MD NN VHP'' with a likelihood of $0.1429$.

Rules are extracted by first finding the location of the reordering rule and then putting the placeholder. Depends on the location, where the placeholder is put, and how much the placeholder replace,  the long rules also have some varieties:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Left/right rules:} depends on if the placeholder is put on the left part or right part
\item \textbf{All/part replacement:} depends on if all the words in a part is replaced by the placeholder
\end{itemize}

\subsection{Tree Rules}
\label{treerules}

While short rules and long rules are based on the flat structure of the sentence, tree rules reorders the sentence by using information from the sentence's syntactic structure. The syntax tree and alignment of the training corpus are used to train the rules. The tree rules reorder the words both on the word level and on the constituent level. Following is an example:
$$\verb|NP ( ADJP JJ NN ) -> JJ NN ADJP (0.06)|$$
The parenthesis in the example represents the hierarchies in the syntax tree. The left side of the rule corresponds a tree with root labeled with ``NP'' and three children, each labeled with ``JJ'', ``NN'' and ``ADJP''. When this structure appears as a subtree in the syntax tree of the sentence to translate, the order of its subtrees should be changed. The change is illustrated in figure $f$.

\begin{figure}[H]
\centering
\input{swap.tikz}
\caption{Change subtree order based on tree rules}
\end{figure}

Tree rules also have some different varieties:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Partial Rules:} the relatively flat syntactic structure of languages like German may make the rule extraction difficult, because the extraction requires that the whole subtree including all its children is matched. In order to extract more useful information for reordering, rules are also extracted from any partial child sequence in a constituent.
\item \textbf{Recursive rule application:} the rules may be applied recursively to already reordered sentence. And all paths of the reorderings are added to the lattice.
\end{itemize}


\subsection{Rule Extraction and Application}
\label{general}

Rules are extracted by scanning all the training data and detecting the word order change. A valid word order change that can count for reordering rule needs to fulfill the two criteria in general:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Order change exists:} otherwise, there's no need for reordering rules.
\item \textbf{No \hyperref[alignedrange]{aligned ranges} coincidence:} the coincidence makes it hard to decide the new word orders in target language.
\end{itemize}

Rules are not always extracted upon discovering of word change. In order to avoid too excessively concrete rules which don't apply well in general, we extract reordering rules only when the same reordering pattern appears more than a certain threshold. Thus it won't lead to overfitting.

The associated probabilities of reordering rules is the frequency how often the sequence in the rules are reordered in the same manner. For example, if the sequence ``after the accident'' appears many times in the training data, and half of the time, it's reordered as ``the accident after'', then the probability of the reordering rule is calculated as $50\%$.

Rules are applied by scanning the text to be translated. When there's a sequence coincides the left side of the reordering rules, rules will be applied, and a path in the \hyperref[ch:Foundations:sec:Lattices]{word lattice} representing the reordered word will be added.

\subsection{Rule Combination}

In order to further explore the probability of improvement, we combine different rules for reordering in our experiment. This is done by training the different types of rules separately and applying them on the monotone path of the sentence independently. They result different paths in the lattice.



%% ===========================
\section{Oracle Reordering}
\label{ch:Foundations:sec:oracle}
%% ===========================



%% ===========================
\section{Word Lattice}
\label{ch:Foundations:sec:Lattices}
\label{latticecreation}
%% ===========================
A word lattice could be presented with a directed acyclic graph. The graph contains nodes and transitions, with each transition labeled with a word and a probability. The outgoing transitions from a node indicate different options, which words could come after this point. The annotation on the transition indicate the word that could come, together with the probability of this option. The word lattice groups different word reorderings of the same English sentence together, with each reordering corresponding a path from the beginning node to the end node. 

Word lattice for reordered word sequences is build gradually while applying the reordering rules on the sentence. It starts with a monotone path presenting the sentence to translated. Every time when a rule is applied and part of the sentence is reordered. We add a parallel path to the corresponding part of the initial monotone path. The parallel path is labeled with reordered words on its transitions. The probability of this new reordering is subtracted from the first transition after the splitting point on monotone path, and assigned to the first transition of the new path. All the other transitions on the new path that follow have a probability of $1$.

%? graph for building?

Paths with very low probability are removed, in order to save space for storing the lattice and reduce decoding time later, without compromising to much translation quality. %?In our system, we remove all the paths that have a probability lower than $0.1$.

An example of a word lattice is showed in figure $2.6$. If the probability of a transition is $1$, it's left out in the example to keep the graph clear.


\begin{landscape}
\begin{figure}
\centering
\input{Lattices.tikz}
\caption{Example of a word lattice}
\end{figure}
\end{landscape}

\section{BLEU Score}
\label{ch:Foundations:sec:bleu}
BLEU is the de facto standard in machine translation\cite{metrics}. We use BLEU score to evaluate the SMT system throughout this paper.


\section{Summary}

%% ===========================
\chapter{Reordering Approach}
\label{ch:ReorderingApproach}
%% ===========================

%% ===========================
\section{Reordering Problem in Chinese Translation}
\label{ch:ReorderingApproach:sec:Problem}
%% ===========================

English and Chinese belong to different language groups. As Chinese belong to the Sino-Tibet language group, while English belong to the Indo-Germanic language group. And they have also developed separately for a long time. Because of their different origins and development, they've becoming two very different languages.

Unlike the most languages in the Indo-Germanic language group, which are more close to English than Chinese, Chinese has some properties those languages don't have, such as Characters as fundamental element instead of letters, the tones, no word separation, the usage of measure words,much more simple inflections and conjugations, which raises further problem for machine translation.

Even the word order between English and Chinese is more distinct. For one, the words in Chinese have generally different origins as those in English, which leads to different vocabulary and sometimes it's very hard to found corresponding words in the other language. For example, Chinese has a lot of different prepositions and adverbs, which have very distinct usage as those in English. Also the continuous writing of Chinese without space makes this problem more severe, since word boundaries are not always so clear in Chinese. Text needs to be segmented before translation, and a word segmentation program decide how to separate the words and the result is not always ideal.

For the other, 

And sometimes, a long English sentence with clauses is more suitable to be translated into two of more Chinese sentences. Through analyzing the data we have, we've found several major word order differences between English and Chinese, which leads to low translation quality and should be changed. 

%? examples of different kinds for word order differences

\cite{syntactic} also systematic analyzing about Chinese syntactic reordering. Through our analyzing / research, examples below, worth noticing / improving

``Quote'':


Mandarin Chinese sentence structure is quite different than English or other European languages. Since the word order doesn't match, sentences which are translated word-for-word to Mandarin will be difficult to understand. You must learn to think in Mandarin Chinese when speaking the language. 

There're plenty of literatures discussing the sentence strucutes of chinese cite.. please.

\paragraph{Preposition phrases}

\paragraph{relation clauses}

\paragraph{Adverbials}
An adverbial can be an adverb, an adverbial phrase or an adverbial clause that modifies the verb or the whole sentence. The position of adverbials is a complicated topic. In general, the location of adverbials in a sentence are very flexible. They can be located in different position of a sentence both in English and Chinese, such as beginning, middle or end of a sentence, before or after the verb. The location varies and it often depends on the situation. When comparing English and Chinese word order, the location of adverbials in one language doesn't automatically implies the same location in the other. Typical examples are adverbials of time, location and frequency. It's often put after the verb in English, but before the verb in Chinese. 
\begin{figure}[H]
\centering
%\input{adverbial.tikz}
\caption{Position change of an adverbial of time}
\end{figure}


\paragraph{questions}

\paragraph{There be ...}
%?correct compound subjects: or & there be...
%% ===========================
\section{Motivation of Reordering on Multiple Syntactic Levels}
\label{ch:ReorderingApproach:sec:Motivation}
%% ===========================


%? long distance position change, unstructural position change.

\subsection{Long Distance Word Reordering}

efficient than hier

use tree and alignment only

\subsection{Unstructured Word Reordering}

Because English and Chinese have different word orders and there're also some special cases of sentence patterns, the word reordering can be complicated and unsystematically. Sometimes it involves word position changes on multiple syntactic levels. We can see this problem from the examples in figure $3.1$. In the figure, the syntactic structure and alignment of the parallel text are presented in a intuitive way.

\begin{figure}[H]
\centering
\subfigure {
\input{unstructured.tikz}
}
\subfigure {
\input{unstructured2.tikz}
}
\caption{Examples of reordering on multiple syntactic levels}
\end{figure}

From the examples we can also clearly see how to reordering on multiple syntactic levels works. From the root of the syntax tree on the left, if we inspect 3 syntactic levels downwards, we can find the following pattern of word position change, comparing the parallel Chinese text:
$$\texttt{NP ( {\color{blue}CD} NP ( NP ( {\color{blue}JJ NNS} ) PP ( {\color{blue}IN NP} ) ) ) -> NP IN CD JJ NNS}$$
As we can see, we can't get this pattern if we only observing subtrees on the same level of syntax tree. The node labeled with ``CD ten'' is inserted into the subtree of its sibling labeled with ``NP'', between its sibling's two children. This position change can not simply be done by swapping children of the same node.

We can also observe the same phenomenon from the second example. If we inspect two levels downwards from the root node, we can find the following pattern:
$$\texttt{S ( NP ( {\color{blue}NP PP} ) VP ( {\color{blue}VBP ADVP}) {\color{blue}.} ) -> ADVP NP PP VBP .}$$
In this example, the adverbial phrase ``annually'' is moved forward to the front of the sentence, leaving the subtree ``NP ( NP PP )'', which is on a higher level, between it and its sibling.

So as a whole, there are several reasons why we need word reordering on multiple syntactic levels. First, the syntactic parser may make mistakes. As we found in the training corpus, it's not rare that sentences are misparsed, either the words are not correctly tagged, or the syntactic structure is wrong. Second, the syntax tree of the English sentence may not be suitable for translation into Chinese. In these case, MLT reordering could be used as a remedy for incorrect parsing patterns in the syntax tree. Besides due to the very different word orders between English and Chinese, simply reordering the words by change nodes on the same syntax tree level may not do the job, and MLT reordering will be useful in this case.

%% ===========================
\section{The SMT Reordering Algorithm}
\label{ch:ReorderingApproach:sec:Algorithm}
%% ===========================

Inspired the method of tree rules based of reordering and hierarchical SMT model, we created this reordering algorithm. The algorithm solely uses information of the syntax tree and alignment of the source side. It further explores the syntactic structure of text to be translated, and detecting reordering patterns based on multiple hierarchies of the syntax tree.

As we've ready seen how the basic idea of finding reordering patterns on multiple syntactic levels generally works in the last section. We'll systematically explain the rule extraction and application in all details in this chapter.

\subsection{Rule Extraction}

In order to find as much as information for reordering as possible. The algorithm of rule extraction detects the reordering pattern from all nodes in the syntax tree, goes downwards for any number of hierarchies, until it reaches the last hierarchy in the subtrees.

In the implementation, the program conducts a depth-first search (DFS) to traverse every node in the syntax tree. Every time when a node is reached, the program conducts another iterative deepening depth-first search (IDDFS) in its subtree with depth-limit from $1$ to the subtree's depth. And the program detects if there're any patterns of word position changes at the same time, by using the alignment for comparison.

The detected word position changes are checked for their validity for reordering rules. As describe in \hyperref[general]{section $2.6.4$}, a valid patterns for reordering should both be involved with actual reordered words and have clearly distinguished new order from the target side, i.e. no coincidence of aligned range on the target side.

\begin{figure}[H]
\centering
\input{extract.tikz}
\caption{Illustration of rule extraction}
\end{figure}

Figure $3.2$ shows a phrase to be translated together with is syntax tree and alignment to its parallel text in Chinese. The nodes are labels with numbers. In this example, we can find the following reordering patterns:

From node $1$:\\
\texttt{NP ( {\color{blue}NP PP} ) -> PP NP} \hfill [1 level]\\
\texttt{NP ( NP ( {\color{blue}JJ NNS} ) PP ( {\color{blue}IN NP} ) ) -> NP IN JJ NNS} \hfill $*$[2 levels]\\
\texttt{NP ( NP ( {\color{blue}JJ NNS} ) PP ( {\color{blue}IN} NP ( {\color{blue}JJ NNS} ) ) ) -> JJ NNS IN JJ NNS} \hfill [3 levels]

From node $3$:\\
\texttt{PP ( {\color{blue}IN NP} ) -> NP IN} \hfill [1 level]\\
\texttt{PP ( {\color{blue}IN} NP ( {\color{blue}JJ NNS} ) ) -> JJ NNS IN} \hfill $*$[2 levels]

It's noteworthy that our method can detect more reordering patterns than the tree rule based method. For example, the patterns marked with $*$ above can not be extracted with the tree rules based method directly.

The probability of the reordering patterns can be calculated as described in \hyperref[general]{section $2.6.4$}. There are the left part and the right part of the reordering patterns separated by the arrow. The left part indicates the sequence that should be reordered and the right part indicates how the new order should be like. The probability of the pattern is calculated by how often the left part is reordered into the right part among all its appearance in the training corpus. In addition, reordering patterns that appear less than a threshold are ignored to be used as reordering rule, in order to prevent too concrete rules without generalization capability and overfitting.

\subsection{Rule Application}

The syntax tree is traverse by DFS as the same in rule extraction. But from the root of each subtree, it's scanned with depth limit from its maximal levels, i.e. its depth, to $1$. If it turns out, any rule can be applied for a subtree at some level, a new path for this reordering will be added to the word lattice for decoding, as introduced in \hyperref[latticecreation]{section $2.8.1$}. As long as rules can be applied on a subtree for a certain depth, the rules are applied and the search for rule application on this subtree stops, and the search on the next subtree continues. 

The reason for this is to prevent duplicate reorderings due to application of rules, which has overlapped effect with each other. These rules are normally patterns that are generated on the same tree, but with different number of levels, which has different generalization effect on the same range of words in the text. For exmaple, the following patterns can be detected from the syntax tree in figure $3.2$ in last section:
\begin{center}
\begin{tabular}{l}
\texttt{PP ( {\color{blue}IN NP} ) -> NP IN}\\
\texttt{PP ( {\color{blue}IN} NP ( {\color{blue}JJ NNS} ) ) -> JJ NNS IN}
\end{tabular}
\end{center}
Both patterns are detected from the same node, but the second pattern is detected by retrieving the nodes one level deeper and it's more concrete. So the first pattern can be seen as a generalization of the second pattern. Whenever a rule of the second pattern can be applied, a rule of the first pattern can be applied too. Because subtrees are checked from the highest number of levels by rule application, the more concrete rule is applied first. Because the more concrete rule fits the detected pattern better and contains more details of reordering, so it may be more suitable for rule application. In this example, the second rule is applied rather than the first rule.

To illustrate how the rule application work in a more intuitive way, we present an example. Assume we have several reordering rules and a pre-processed sentence for reordering as follows:
\begin{center}
\begin{tabular}{l}
Rules:\\
\text{[1]} \texttt{VP ( {\color{blue}VBZ} NP ( {\color{blue}NP PP} ) ) -> PP VBZ NP (0.18)}\\
\text{[2]} \texttt{NP ( NP ( {\color{blue}NN-1 NN-2} ) PP ( {\color{blue}IN NP} ) ) -> NP IN NN-1 NN-2 (0.17)}\\
\\
Sentence:\\
\texttt{world bank plans debt relief for poorest countries}
\end{tabular}
\end{center}

The syntax tree and monotone path as initial word lattice looks as follows:
\begin{figure}[H]
\centering
\subfigure{
\input{left1.tikz}
}
\subfigure{
\input{right1.tikz}
}
\caption{Illustration of rule application (1)}
\end{figure}

By using DFS to traverse the syntax tree, the program first finds out the pattern started from node ``VP'' with $2$ levels corresponds the left part of the first rule listed above. This indicates the rule is applicable at this position. According to the reordering rule, the order of the three constituents labeled with ``VBZ'', ``NP'' and ``PP'' should be changed to ``PP VBZ NP'' with probability $0.18$. Thus the words are reordered into ``for poorest countries plans debt relief'', and the new path with this probability is added to the word lattice.

\begin{figure}[H]
\centering
\subfigure{
\input{left2.tikz}
}
\subfigure{
\input{right2.tikz}
}
\caption{Illustration of rule application (2)}
\end{figure}

As the program keeps going, it turns out the second rule is applicable at another node labeled with ``NP'' with $2$ levels. Again the rule is applied with probability $0.17$ and the new path is added. It should be noted that there're numbers added to the two ``NN'' tags in the rule, in order to distinguish them for the reordering. The program uses index for the internal presentation of words, which doesn't cause any confusion.

\begin{figure}[H]
\centering
\subfigure{
\input{left3.tikz}
}
\subfigure{
\input{right3.tikz}
}
\caption{Illustration of rule application (3)}
\end{figure}

\section{Summary}

advantages