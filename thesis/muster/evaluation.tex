%% evaluation.tex
%%

%% ==================
\chapter{Evaluation}
\label{ch:Evaluation}
%% ==================

We've conducted two sets of experiments to test our reordering methods. The first set of experiments is designed for testing the English-to-Chinese translation, which is described in section~\ref{ch:Evaluation:sec:enw}. The second set of experiments is designed for the Chinese-to-English translation direction, which is described in section~\ref{ch:Evaluation:sec:zhen2}. Through the experiments on both translation directions, we could get a better overview of the \ac{MLT} reordering's effect.

Both sections are composed of two parts: experimental setup, results. The first part describes details of the system configurations and experimental data. The second part shows the results of different systems for comparison. And section~\ref{ch:Evaluation:sec:Evaluation} evaluates the translation systems in overall.


\section{English to Chinese System}
\label{ch:Evaluation:sec:enw}

\subsection{Experimental Setup}
We performed experiments with or without different reordering methods covering the English-to-Chinese translation direction. The reordering methods included our \ac{MLT} reordering approach and the existing reordering approaches with short rules, long rules and tree rules. The system was trained on news text from the \acs{LDC} corpus and subtitles from \acs{TED} talks. The development data and test data were both news text from the \acs{LDC} corpus. The system was a phrase-based \ac{SMT} system, which used a $6$-gram language model with Knersey-Ney smoothing. Besides the preordering, no lexical reordering or other reordering method in decoding phase was used. The text was translated through a monotone decoder. The Chinese text were first segmented into words with the \emph{Stanford Word Segmenter}\footnote{Stanford Word Segmenter: \url{http://nlp.stanford.edu/software/segmenter.shtml}} before use.

The reordering rules were extracted by using the word alignments, POS tags and syntax trees from the training data. One reference of the test data was used for evaluating the results. The threshold for rule extraction is set as $5$ times and reordering paths with probability less than $0.1$ are not added to word lattices. The decoder was a monotone decoder. Table~\ref{denw} shows the size of data used in the system. %? case-sensitive?

\begin{table}[H]
\centering
\begin{tabular}{|ll|r|r|r|r|r|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{Data Set}} & \multirow{2}{*}{Sentence Count} & \multicolumn{2}{c|}{Word Count} & \multicolumn{2}{c|}{Size (Byte)}\\ \cline{4-7}
& & & English & Chinese & English & Chinese \\
\hline
\multirow{2}{*}{Training Data} & \multicolumn{1}{|l|}{LDC} & 303K & 10.96M & 8.56M & 60.88M & 47.27M \\ \cline{2-7}
& \multicolumn{1}{|l|}{TED} & 151K & 2.58M & 2.86M & 14.24M & 15.63K \\ \hline
\multicolumn{2}{|l|}{Development Data} & 919 & 30K & 25K & 164K & 142K \\ \hline
\multicolumn{2}{|l|}{Test Data} & 1663 & 47K & 38K & 263K & 220K \\ \hline
\end{tabular}
\caption{Data details in English-to-Chinese system}
\label{denw}
\end{table}

\subsection{Results}

\begin{table}[H]
\centering
\STautoround*{2}
\begin{spreadtab}{{tabular}{|l|r|r|r|}}\hline
@				& @BLEU Score (\%) & @Improvement & @TER (\%) \\ \hline
@Baseline		& 12.07 & & 72.15 \\ \hline
@+Short Rules	& 12.50 & :={b3 - b2} & 71.41 \\ \hline
@+Long Rules   & 12.99 & :={b4 - b2} & 70.71 \\ \hline
@+Tree Rules   & 13.38 & :={b5 - b2} & 68.27 \\ \hline
\textbf{@+MLT Rules} & \textbf{:={13.81}} & \textbf{:={b6 - b2}} & \hphantom{xxx}\textbf{:={68.20}} \\ \hline
@Oracle Reordering & :={18.58} & :={b7 - b2} & :={62.13} \\ \hline
\hline
@Long Rules   & 12.31 & :={b8 - b2} & 71.81\\ \hline
@Tree Rules   & 13.30 & :={b9 - b2} & 70.42 \\ \hline
\textbf{@MLT Rules}    & \textbf{:={13.68}} & \textbf{:={b10 - b2}} & \textbf{:={70.25}} \\ \hline
\end{spreadtab}
\caption{Result overview of English-to-Chinese system}
\label{tenw}
\end{table}

Table~\ref{tenw} shows the \ac{BLEU} scores, absolute improvements of \ac{BLEU} scores and \ac{TER}s for configurations with different reordering methods. The table consist of $2$ sections. the first row of the top section shows results of the baseline, which used no preordering at all. In the following rows of the top section, different types of reordering rules are combined gradually, with each type per row, and the results are showed. For example, the row with \emph{$+$MLT} Rules presents the configuration with all the rule types including \ac{MLT} rules and all the other rules in the rows above. All improvements are absolute improvements of \ac{BLEU} scores in comparison to the baseline. Each row with a certain reordering type presents all the different variations of this type and the best score under these configurations are shown. For example, long rules include the left rules and right rules, and the tree rules include the partial rules and recursive application. In the lower section of the table, different rule types are not combined and the effect of each rule type is shown. %\ac{TER} of each configuration is also listed in the last column. %? Besides, the lower section also shows the total number of rules that are extracted and applied for each rule type. (Problem: many variations :(  )



%% ===============================
\section{Chinese to English System}
\label{ch:Evaluation:sec:zhen2}
%% ===============================


\subsection{Experimental Setup}

The experiments for Chinese-to-English systems had a similar setup as described in the last section. The parallel data used in the English-to-Chinese system was also used in this experiment by switching the source language and the target language. We only used the \acs{LDC} data set for training, and no \acs{TED} data were used in this system. The test data had three English references for evaluating the results instead of one as in the previous system. The data used are summarized in table~\ref{dzhen2}.


\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\multirow{2}{*}{Data Set} & \multirow{2}{*}{Sentence Count} & \multicolumn{2}{c|}{Word Count} & \multicolumn{2}{c|}{Size (Byte)}\\ \cline{3-6}
& & Chinese & English & Chinese & English \\
\hline
Training Data & 303K & 8.56M & 10.96M & 47.27M & 60.88M\\ \hline
Development Data & 919 & 25K & 30K & 142K & 164K \\ \hline
Test Data & 1663 & 38K & 47K & 220K & 263K \\ \hline
\end{tabular}
\caption{Data details in Chinese-to-English system}
\label{dzhen2}
\end{table}

\subsection{Results}

\begin{table}[H]
\centering
\STautoround*{2}
\begin{spreadtab}{{tabular}{|l|r|r|r|}}\hline
@				& @BLEU Score (\%) & @Improvement & @TER (\%) \\ \hline
@Baseline		& 21.80 & & 62.09 \\ \hline
@+Short Rules	& 22.90 & :={b3 - b2}& 61.64 \\ \hline
@+Long Rules   & 23.13 & :={b4 - b2} & 61.43\\ \hline
@+Tree Rules   & 23.84 & :={b5 - b2} & 60.95\\ \hline
\textbf{@+MLT Rules}    & \textbf{:={24.14}} & \textbf{:={b6 - b2}} & \hphantom{xxx} \textbf{:={60.79}}\\ \hline
@Oracle Reordering & :={26.80} & :={b7 -b2} & :={56.97} \\ \hline
\hline
@Long Rules   & 22.10 & :={b8 - b2} & 62.21\\ \hline
@Tree Rules   & 23.35 & :={b9 - b2} & 61.52\\ \hline
\textbf{@MLT Rules}    & \textbf{:={23.96}} & \textbf{:={b10 - b2}} & \textbf{:={60.83}}\\ \hline
\end{spreadtab}
\caption{Result overview of Chinese to English systems}
\label{tzhen2}
\end{table}

Table~\ref{tzhen2} shows the results for configurations with different reordering methods for the Chinese-to-English translation. The table can be interpreted in the same manner as table~\ref{tenw} in the previous section.

\section{Evaluation}
\label{ch:Evaluation:sec:Evaluation}

The results shows increasing scores as we used reordering methods from short rules, long rules, tree rules to \ac{MLT} rules. And better \ac{BLEU} scores were achieved as we combined the different reordering rules. The \ac{MLT} rules achieved better \ac{BLEU} scores and \ac{TER}s on both translation directions, not only when it was used alone, but also it was added to the other reordering rules. When our \ac{MLT} reordering rules were combined with the other existing reordering rules, a further improvement of $0.43$ in \ac{BLEU} score (from $13.38$ to $13.81$) was shown in the English-to-Chinese translation direction, and a further improvement of $0.3$ in \ac{BLEU} score (from $23.84$ to $24.14$) was shown in the Chinese-to-English translation direction.

We've also found improvements in the sentence structure. Table~\ref{te} shows some translation examples on both translation directions. Sections are separated by double lines in the table. Each section of this table shows one translation example with the source sentence (\emph{source}), translation without using MLT reordering (\emph{no MLT}), translation with MLT reordering (\emph{MLT}) and the reference (\emph{reference}). The translation without using \ac{MLT} reordering comes from the configuration with highest \ac{BLEU} score that didn't use \ac{MLT} reordering. And the translation with \ac{MLT} reordering comes from the configuration with highest \ac{BLEU} score that used \ac{MLT} reordering. From the examples, we can clearly see the improvements in sentence structure.

\begin{table}
\centering
\begin{tabular}{|l|m{0.7\textwidth}|} \hline
Source & hu jintao also extended deep condolences on the death of the chinese victims and expressed sincere sympathy to the bereaved families .
\\ \hline
No MLT & \cntext{胡锦涛 还 表示 深切 哀悼 的 受害者 家属 的 死亡 , 向 迂难者 家属 表示 诚挚 的 慰问 。} \\ \hline
MLT & \cntext{胡锦涛 还 对 中国 迂难者 表示 哀悼 , 向 迂难者 家属 表示 诚挚 的 慰问 。} \\ \hline 
Reference & \cntext{胡锦涛 还 对 中方 不幸 遇难 人员 表示 深切 的 哀悼 , 并 向 遇难 着 的 亲属 致以 诚挚 的 慰问 。
} \\ \hline \hline

%Source & The plaintiffs' attorney pointed out that this ruling ``violates the historical record'', and demanded that the Japanese government and Nippon Yakin pay compensations and offer apologies to resolve the issue . \\ \hline
%No MLT & \cntext{这 个 原告 的 律师 指出 , 这 种 `` 违背 了 历史 记录 '' , 要求 日本 政府 , 正 yakin 支付 补偿 , 道歉 来 解决 问题 。} \\ \hline
%MLT & \cntext{原告 的 律师 指出 , 这 一 判决 `` 违背 了 历史 纪录 '' , 要求 日本 政府 , 正 yakin 支付 补偿 , 道歉 来 解决 问题 。} \\ \hline \hline

Source & satisfying personal interests and expanding knowledge are also major reasons why hourly work appeals to people .\\ \hline
No MLT & \cntext{满足 个人 利益 和 扩大 知识 也 是 主要 原因 小时 工作 吸引 人 。} \\ \hline
MLT & \cntext{满足 个人 利益 和 扩大 知识 也 是 为什么 学生 工作 吸引 人 的 主要 原因 。} \\ \hline 
Reference & \cntext{满足 个人 兴趣 , 扩大 自己 的 知识面 也 是 兼职 小时 工 受 青睐 的 一 个 重要 原因 。} \\ \hline \hline

Source & the dalai lama will go to visit washington this month .\\ \hline
No MLT & \cntext{达赖 喇嘛 将 访问 华盛顿 的 这 一 个 月 。} \\ \hline
MLT & \cntext{达赖 喇嘛 将 本 月 访问 华盛顿 。} \\ \hline
%? add more examples?
Reference & \cntext{达赖 喇嘛 将 在 本 月 前往 华盛顿 访问 。} \\ \hline \hline

%Source & \cntext{防务 报告 与 军演 之间 的 微妙 关系 成为 讨论 的 交点 之一 。}\\ \hline
%No MLT & report on defense and military exercise of the delicate relations between become the \cntext{交点} discussion .
%\\ \hline
%MLT & the delicate relations between defense report and military exercise has become the \cntext{交点} discussion .\\ \hline
%Reference & the subtle relationship between the defense report and the military exercise has become one of the focuses of discussion .\\ \hline \hline

Source & \cntext{陈至立 说 , 古巴 是 拉美 和 加勒比 地区 有 重要 影响 的 国家 。}
\\ \hline
No MLT & chen zhili said : cuba is the latin america and the caribbean region has an important influence on the state .
\\ \hline
MLT & chen zhili said : cuba is a country of important influence latin america and the caribbean region .
\\ \hline
Reference & chen zhili said that cuba is a country of great influence in the latin american and caribbean region .
\\ \hline \hline

Source & \cntext{近年 来 , 两 国 教育 交流 日益 密切 , 人员 来往 频繁 。}
\\ \hline
no MLT & in recent years , the two countries education have been increasingly close exchanges and personnel contacts have been frequent .
\\ \hline
MLT & in recent years , the educational exchanges between the two countries have become increasingly frequent , and have had frequent contacts .
\\ \hline
Reference & in recent years , the educational exchange between the two countries has become increasingly close with frequent personnel visits .
\\ \hline

\end{tabular}
\caption{Examples of translations}
\label{te}
\end{table}
%? change the example, it's wrong
%Each section of table $4.3$ shows translation of a sentence in its source language and target language. The translation in the row with ``No MLT'' comes from the configuration without using MLT reordering, and the translation in the row with ``MLT'' comes from the configuration with using MLT reordering. From the examples, we can see that the MLT reordering improved the sentence structure significantly. In the last example, the part of the sentence ``visit Washington this month'' was parsed as the structur ``(visit (Washington) ((this) (month)))'' in the syntax tree, but the correct Chinese translation has word order corresponds to ``this month visit Washington''. This reordering inserts the word ``visit'' between the words ``this month'' and ``Washington'', which are on a lower level of the syntax tree. This behavior could not be done by the tree-rule-based reordering, which only changes order of constituents on the same tree level.
%
%%picture?


However, by taking a close look at the gap between the scores of oracle reordering and the best scores achieved by \ac{MLT} reordering, we can also see, there's still potential for improvements of translation between English and Chinese through better reordering methods.

From this experiments we can draw the conclusion that our reordering method obviously improves the sentence strucuture and  translation quality in both English-to-Chinese and Chinese-to-English translation directions, no matter when we apply it alone or when we combine it with the reordering methods based on short rules, long rules and tree rules. 


%And we could further justify our claim that the MLT reordering method changes the word order more significantly to improve the translation quality.


\section{Summary}
\label{ch:Evaluation:sec:Conclusion}

In this chapter, we've presented and evaluated the results of the English-to-Chinese and Chinese-to-English \ac{SMT} system. In both system, our \ac{MLT} reordering method shows obvious improvements on translation quality.

When our \ac{MLT} reordering rules were combined with the other existing reordering rules, a further improvement of $0.43$ in \ac{BLEU} score was shown in the English-to-Chinese translation direction, and a further improvement of $0.3$ in \ac{BLEU} score was shown in the Chinese-to-English translation direction.

Through analyzing the syntactic structure of the sentences closely, we've also found that the \ac{MLT} reordering method improved the translation by changing the order of words, not only on the same syntax tree level but also between different levels, which could not be easily achieved by other reordering methods we've introduced so far. So it further justifies our claim, that the \ac{MLT} reordering method improved the sentence structure obviously and led to better translation between English and Chinese.
