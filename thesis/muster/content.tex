%% content.tex
%%

%% ===========================
\chapter{Foundations}
\label{ch:Foundations}
%% ===========================

This chapter provides an introduction to fundamental knowledge that are relevant to this work. We start with the whole SMT system and pre-reordering system first, then followed by the information we used to create the reordering rules including alignment, POS tag, syntax tree. At the end we shows the different rule types, the oracle reordering and how to build lattices for translation. \cite{book} also provide a good introduction to statistical machine translation, including different kinds of theories and methods that are relevant to this work.

%% ===========================
\section{SMT System}
\label{ch:Foundations:sec:SMTSystem}
%% ===========================

%Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.\cite{wikismt} One typical archit

Statistical machine translation (SMT) is the state of art machine translation paradigm. It uses a typical log-linear model which is composed of a decoder and different statistical models including phrase table, reordering model and language model. All the models are weighted with parameters which are tuned from the development data. Besides development data, training data are used for training the alignment, phrase table and other models. And test data are used for evaluation purpose. The architecture of a SMT system could be illustrated as figure $2.1$.

\begin{figure}[H]
\centering
\input{smt.tikz}
\caption{Architecture of SMT system}
\end{figure}

%% ===========================
\section{Rules Based Pre-Reodering}
\label{ch:Foundations:sec:PreReorderingSystem}
%% ===========================

Our pre-reordering method is based on reordering rules. Reordering rules are rules that tell us how we should reordering the sentences in source language before translating them. In our system, the rules are generated by using the word alignment, part-of-speech (POS) tag and syntax tree, all of which are calculated based on the training data. After we apply the reordering rules to the source sentences, word lattices are generated. The word lattices contains all the reordering possibilities of the source sentences and are further passed to the decoder for translating. The pre-reodering system could be illustrated as figure $2.2$.

\begin{figure}[H]
\centering
\input{prereordering.tikz}
\caption{Pre-reordering system}
\end{figure}

A more detailed description of \hyperref[ch:Foundations:sec:Alignment]{word alignment}, \hyperref[ch:Foundations:sec:PosTag]{POS tag}, \hyperref[ch:Foundations:sec:SyntacticTree]{syntax tree},
\hyperref[ch:Foundations:sec:types]{reordering rules} and 
\hyperref[ch:Foundations:sec:Lattices]{word lattices} is also clarified in the following sections.

The \hyperref[ch:ReorderingApproach]{reordering approach} we used for extracting and applying the rules is introduced in the next chapter.

%% ===========================
\section{Word Alignment}
\label{ch:Foundations:sec:Alignment}
%% ===========================

Word alignment indicate the possible alignment between words in the source sentence and words in the target sentence. For example, figure $2.3$ shows an alignment between an English sentence and a Chinese sentence.

\begin{figure}[H]
\centering
\input{alignment.tikz}
\caption{Example of word alignment}
\end{figure}
%? better example with crossing alignment?
Or it may be simply presented as index pairs.
\begin{center}
\verb|7-1 8-1 8-2 9-3 9-4 1-5 1-6 2-6 5-6 3-7 4-8 4-9 10-10|
\end{center}

In the figure, the words that are aligned through lines in two languages have related meaning. We can see how the words are reordered from the figure. For exmaple, the noun clause ``Clinton's presidency'' is completely moved forward to the front of the sentence.

\phantomsection\label{alignedrange}
It is noteworthy that the alignment is not always a one-to-one match. In the example, the word ``of'' is not aligned at all and the word ``months'' is aligned to two word ``\cntext{个}'' and ``\cntext{月}'' in Chinese. We can define the aligned range as the range from the first word a certain word is aligned to to the last word it is aligned to. For example, the aligned range of word ``there'' is $[5,6]$. The \textbf{aligned range} can be coincided, such as the $6$th word ``\cntext{剩下}'' in Chinese is also aligned to ``are'' and ``left'' at the same time besides ``there''. The coincidence sometimes makes the detection of rules more difficult, because the word order is not clear any more.

The word alignment could be trained with the GIZA++ tool by using Expectation Maximization (EM) algorithm. From the word alignment of training data we can see the patterns how the words are reordered before and after translation. Therefore, we could extract these reordering rules and apply them on the text, which is to be translated.

%% ===========================
\section{Part-of-Speech Tag}

Part-of-speech (POS) tags are markups of words in the text, which corresponds their linguistic role in the text. Depends on the definition of the roles, the set of POS tags could be different. Besides, different languages may have different POS tag set, since they may have different linguistic features, which are relevant to translation.
\begin{figure}[H]
\centering
\input{tags.tikz}
\caption{Example of POS tags}
\end{figure}
Figure $2.4$ shows an tagged English sentence. 

Tagset? English \& Chinese how to tag?


\label{ch:Foundations:sec:PosTag}
%% ===========================

%% ===========================
\section{syntax tree}
\label{ch:Foundations:sec:SyntacticTree}
%% ===========================

The syntax tree shows the syntactic structure of a sentence and can be very useful for word reordering. A syntax tree contains two kinds of nodes: the leaves and the internal nodes. Each leaf presents a word in the sentence, and is annotated with the POS tag. And each internal node presents a constituents, which is also annotated to indicate its category or syntactic role. In the Penn treebank \citep{penn}, for example, the annotation ``NP'' means noun phrase and the annotation ``S'' means simple declarative clause. Figure $2.5$ is an example of a syntax tree.

\begin{figure}[H]
\centering
\input{ParseTree.tikz}
\caption{Example of a parse tree}
\end{figure}

We can see the syntactic structure of the sentence from the syntax tree very clear. In this example, The words ``math and biology exams'' make up a noun clause, which plays the roll of subject. The predicate has a nested structure of verb clause, because it's compound with the modal verb ``will'' and the word ``be''. And ``on the 27th'' is a preposition clause in the verb clause, which is again composed of a preposition ``in'' and a noun clause ``the 27th''.

%%% ===========================
%\section{Dependency Tree}
%\label{ch:Foundations:sec:DependencyTree}
%%% ===========================

%% ===========================
\section{Reordering Rules}
\label{ch:Foundations:sec:types}
%% ===========================
%Short rules was introduced by \cite{short}.
%\subsection{Short Rules}
%\subsection{Long Rules}
%\subsection{Tree Rules}

Based on \cite{short}, \cite{long}, \cite{tree} and \cite{combine}, we introduce the different rule types, rule combination, how to decide if rules can be extracted as well as how to calculate the probability to apply the rules.

\subsection{Short Rules}

Short rules are extracted based on the sequences of adjacent words or their POS tags in the sentence to translate. Sequences of adjacent words or tags are observed, rules are then extracted if the same reordering patterns appear frequently. Following are some examples:
$$\verb|after the accident -> the accident after (0.5)|$$
$$\verb|WRB MD DT -> DT WRB DT (0.3)|$$
The first rule in this example shows, if a the word sequence ``after the accident'' ever appears in the text, it should be reordered to ``the accident after'' with a probability $0.5$, so will the word order be more consistent with the translation. The second rule shows, if the word sequence of a wh-adverb (when, where, why, etc.), a modal verb (MD) and a determiner (DT) appears, the determiner should be moved before the wh-adverb with a probablity of $0.3$.

In addition, Short rules also have some different varieties \citep{short}:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Tag sequence:} rules are extracted based on adjacent tag sequence
\item \textbf{Word sequence:} rules are extracted based on adjacent word sequence
\item \textbf{Context of one or two tags before and/or after the tag sequence}
\item \textbf{Context of one or two words before and/or after the tag sequence}
\end{itemize}

\subsection{Long Rules}

Long rules are specially designed to help the long distance word reordering for translation between English and German. The rules are based on POS tags of the text, and an example is as following:
$$\verb|NN X MD VHP -> X MD NN VHP (0.14)|$$
The ``X'' in the example is an placeholder, which can replace one or more words. ``VHP'' is the right context, which is sometimes helpful to define the reordering boundary. ``NN'' means noun, ``MD'' means modal verb and ``VHP'' is the word ``have''. In this example, the tag sequence ``NN X MD VHP'' will be reordered as ``X MD NN VHP'' with a likelihood of $0.1429$.

Rules are extracted by first finding the location of the reordering rule and then putting the placeholder. Depends on the location, where the placeholder is put, and how much the placeholder replace,  the long rules also have some varieties:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Left/right rules:} depends on if the placeholder is put on the left part or right part
\item \textbf{All/part replacement:} depends on if all the words in a part is replaced by the placeholder
\end{itemize}

\subsection{Tree Rules}
\label{treerules}

While short rules and long rules are based on the flat structure of the sentence, tree rules reorders the sentence by using information from the sentence's syntactic structure. The syntax tree and alignment of the training corpus are used to train the rules. The tree rules reorder the words both on the word level and on the constituent level. Following is an example:
$$\verb|NP ( ADJP JJ NN ) -> JJ NN ADJP (0.06)|$$
The parenthesis in the example represents the hierarchies in the syntax tree. The left side of the rule corresponds a tree with root labeled with ``NP'' and three children, each labeled with ``JJ'', ``NN'' and ``ADJP''. When this structure appears as a subtree in the syntax tree of the sentence to translate, the order of its subtrees should be changed. The change is illustrated in figure $f$.

\begin{figure}[H]
\centering
\input{swap.tikz}
\caption{Change subtree order based on tree rules}
\end{figure}

Tree rules also have some different varieties:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Partial Rules:} the relatively flat syntactic structure of languages like German may make the rule extraction difficult, because the extraction requires that the whole subtree including all its children is matched. In order to extract more useful information for reordering, rules are also extracted from any partial child sequence in a constituent.
\item \textbf{Recursive rule application:} the rules may be applied recursively to already reordered sentence. And all paths of the reorderings are added to the lattice.
\end{itemize}


\subsection{Rule Extraction and Application}

Rules are extracted by scanning all the training data and detecting the word order change. A valid word order change that can count for reordering rule needs to fulfill the two criteria in general:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Order change exists:} otherwise, there's no need for reordering rules.
\item \textbf{No \hyperref[alignedrange]{aligned ranges} coincidence:} the coincidence makes it hard to decide the new word orders in target language.
\end{itemize}

Rules are not always extracted upon discovering of word change. In order to avoid too excessively concrete rules which don't apply well in general, we extract reordering rules only when the same reordering pattern appears more than a certain threshold. Thus it won't lead to overfitting.

The associated probabilities of reordering rules is the frequency how often the sequence in the rules are reordered in the same manner. For example, if the sequence ``after the accident'' appears many times in the training data, and half of the time, it's reordered as ``the accident after'', then the probability of the reordering rule is calculated as $50\%$.

Rules are applied by scanning the text to be translated. When there's a sequence coincides the left side of the reordering rules, rules will be applied, and a path in the \hyperref[ch:Foundations:sec:Lattices]{word lattice} representing the reordered word will be added.

\subsection{Rule Combination}

In order to further explore the probability of improvement, we combine different rules for reordering in our experiment. This is done by training the different types of rules separately and applying them on the monotone path of the sentence independently. They result different paths in the lattice.



%% ===========================
\section{Oracle Reordering}
\label{ch:Foundations:sec:oracle}
%% ===========================



%% ===========================
\section{Word Lattice}
\label{ch:Foundations:sec:Lattices}
%% ===========================
A word lattice could be presented with a directed acyclic graph. The graph contains nodes and transitions, with each transition labeled with a word and a probability. The outgoing transitions from a node indicate different options, which words could come after this point. The annotation on the transition indicate the word that could come, together with the probability of this option. The word lattice groups different word reorderings of the same English sentence together, with each reordering corresponding a path from the beginning node to the end node. 

An example of a word lattice is showed in figure $2.6$. If the probability of a transition is $1$, it's left out in the example to keep the graph clear.

\paragraph{Building of Lattices Presenting Different Reorderings}

The lattice begins with a monotone path presenting the sentence to translated. Every time when a rule is applied and part of the sentence is reordered. We add a parallel path to the corresponding part of the initial monotone path. The parallel path is labeled with reordered words on its transitions. The probability of this new reordering is subtracted from the first transition after the splitting point on monotone path, and assigned to the first transition of the new path. All the other transitions on the new path that follow have a probability of $1$.

%? graph for building?

Paths with very low probability are removed, in order to save space for storing the lattice and reduce decoding time later, without compromising to much translation quality. In our system, we remove all the paths that have a probability lower than $0.1$.



\begin{landscape}
\begin{figure}
\centering
\input{Lattices.tikz}
\caption{Example of a word lattice}
\end{figure}
\end{landscape}

\section{BLEU Score}
\label{ch:Foundations:sec:bleu}
BLEU is the de facto standard in machine translation\cite{metrics}. We use BLEU score to evaluate the SMT system throughout this paper.


\section{Summary}

%% ===========================
\chapter{Reordering Approach}
\label{ch:ReorderingApproach}
%% ===========================

%% ===========================
\section{Reordering Problem in Chinese Translation}
\label{ch:ReorderingApproach:sec:Problem}
%% ===========================

English and Chinese belong to different language groups. As Chinese belong to the Sino-Tibet language group, while English belong to the Indo-Germanic language group. And they have also developed separately for a long time. Because of their different origins and development, they've becoming two very different languages.

Unlike the most languages in the Indo-Germanic language group, which are more close to English than Chinese, Chinese has some properties those languages don't have, such as Characters as fundamental element instead of letters, the tones, no word separation, the usage of measure words,much more simple inflections and conjugations, which raises further problem for machine translation.

Even the word order between English and Chinese is more distinct. For one, the words in Chinese have generally different origins as those in English, which leads to different vocabulary and sometimes it's very hard to found corresponding words in the other language. For example, Chinese has a lot of different prepositions and adverbs, which have very distinct usage as those in English. Also the continuous writing of Chinese without space makes this problem more severe, since word boundaries are not always so clear in Chinese. Text needs to be segmented before translation, and a word segmentation program decide how to separate the words and the result is not always ideal.

For the other, 

And sometimes, a long English sentence with clauses is more suitable to be translated into two of more Chinese sentences. Through analyzing the data we have, we've found several major word order differences between English and Chinese, which leads to low translation quality and should be changed. 

%? examples of different kinds for word order differences

\cite{syntactic} also systematic analyzing about Chinese syntactic reordering. Through our analyzing / research, examples below, worth noticing / improving

\textbf{Position of adverbs and adverbial clauses}
%% ===========================
\section{Motivation of the Reordering on Multiple Syntactic Levels}
\label{ch:ReorderingApproach:sec:Motivation}
%% ===========================


%? long distance position change, unstructural position change.


efficient than hier

use tree and alignment only

\paragraph{Unstructured Word Reordering}

Because English and Chinese have different word orders and there're also some special cases of sentence patterns, the word reordering can be complicated and unsystematically. Sometimes it involves word position changes on multiple syntactic levels. We can see this problem from the examples in figure $3.1$. In the figure, the syntactic structure and alignment of the parallel text are presented in a intuitive way.

\begin{figure}[H]
\centering
\subfigure {
\input{unstructured.tikz}
}
\subfigure {
\input{unstructured2.tikz}
}
\caption{Illustration of rule extraction}
\end{figure}


$$\verb|NP ( CD NP ( NP ( JJ NNS ) PP ( IN NP ) ) ) -> |$$

So as a whole, there are several reasons why we need word reordering on multiple syntactic levels. First, the syntactic parser may make mistakes. As we found in the training corpus, it's not rare that sentences are misparsed, either the words are not correctly tagged, or the syntactic structure is wrong. Second, the syntax tree of the English sentence may not be suitable for translation into Chinese. In these case, MLT reordering could be used as a remedy for incorrect parsing patterns in the syntax tree. Besides due to the very different word orders between English and Chinese, simply reordering the words by change nodes on the same syntax tree level may not do the job, and MLT reordering will be useful in this case.

%% ===========================
\section{The SMT Reordering Algorithm}
\label{ch:ReorderingApproach:sec:Algorithm}
%% ===========================

%? Our method

The algorithm solely uses information of the syntax tree and alignment of the source side. In the first part of this section, we first explain what are reordering rules from multiple layers of the syntax tree. Afterwards we'll explain how to extract and apply the rules systematically.

\subsection{Reordering Rules from Multiple Layers}

Inspired the method of tree rules based of reordering and hierarchical SMT model, we created this reordering method. In this method we use information from the syntax tree and further explore its syntactic structure. 

\begin{figure}[H]
\centering
\input{extract.tikz}
\caption{Illustration of rule extraction}
\end{figure}




\subsection{Rule Extraction}




\subsection{Rule Application}

\section{Summary}

advantages