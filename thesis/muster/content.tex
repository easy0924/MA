%% content.tex
%%

%% ===========================
\chapter{Foundations}
\label{ch:Foundations}
%% ===========================



%% ===========================
\section{SMT System}
\label{ch:Foundations:sec:SMTSystem}
%% ===========================

%Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.\cite{wikismt} One typical archit

Statistical machine translation (SMT) is the state of art machine translation paradigm. It uses a typical log-linear model which is composed of a decoder and different statistical models including phrase table, reordering model and language model. All the models are weighted with parameters which are tuned from the development data. Besides development data, training data are used for training the alignment, phrase table and other models. And test data are used for evaluation purpose. The architecture of a SMT system could be illustrated as figure $2.1$.

\begin{figure}[H]
\centering
\input{smt.tikz}
\caption{Architecture of SMT system}
\end{figure}

%% ===========================
\section{Rules Based Pre-Reodering}
\label{ch:Foundations:sec:PreReorderingSystem}
%% ===========================

Our pre-reordering method is based on reordering rules. Reordering rules are rules that tell us how we should reordering the sentences in source language before translating them. In our system, the rules are generated by using the word alignment, part-of-speech (POS) tag and syntactic tree, all of which are calculated based on the training data. After we apply the reordering rules to the source sentences, word lattices are generated. The word lattices contains all the reordering possibilities of the source sentences and are further passed to the decoder for translating. The pre-reodering system could be illustrated as figure $2.2$.

\begin{figure}[H]
\centering
\input{prereordering.tikz}
\caption{Pre-reordering system}
\end{figure}

A more detailed description of \hyperref[ch:Foundations:sec:Alignment]{word alignment}, \hyperref[ch:Foundations:sec:PosTag]{POS tag}, \hyperref[ch:Foundations:sec:SyntacticTree]{syntactic tree},
\hyperref[ch:Foundations:sec:types]{reordering rules} and 
\hyperref[ch:Foundations:sec:Lattices]{word lattices} is also clarified in the following sections.

The \hyperref[ch:ReorderingApproach]{reordering approach} we used for extracting and applying the rules is introduced in the next chapter.

%% ===========================
\section{Word Alignment}
\label{ch:Foundations:sec:Alignment}
%% ===========================

Word alignment indicate the possible alignment between words in the source sentence and words in the target sentence. For example, figure $2.3$ shows an alignment between an English sentence and a Chinese sentence.

\begin{figure}[H]
\centering
\input{alignment.tikz}
\caption{Example of word alignment}
\end{figure}

Or it may be simply presented as index pair in the file system.
$$1-1\quad 2-1\quad 2-2\quad 3-3\quad 3-4\quad 4-5\quad 4-6\quad 5-7$$

The word alignment could be trained with the GIZA++ tool by using Expectation Maximization (EM) algorithm. From the word alignment of training data we can see the patterns how the words are reordered before and after translation. Therefore, we could extract these reordering rules and apply them on the text, which is to be translated.

%% ===========================
\section{Part-of-Speech Tag}

Part-of-speech (POS) tags are markups of words in the text, which corresponds their linguistic role in the text. Depends on the definition of the roles, the set of POS tags could be different. Besides, different languages may have different POS tag set, since they may have different linguistic features, which are relevant to translation.
\begin{figure}[H]
\centering
\input{tags.tikz}
\caption{Example of POS tags}
\end{figure}
Figure $2.4$ shows an tagged English sentence. 

Tagset? English \& Chinese how to tag?


\label{ch:Foundations:sec:PosTag}
%% ===========================

%% ===========================
\section{Syntactic Tree}
\label{ch:Foundations:sec:SyntacticTree}
%% ===========================

TODO

\begin{figure}[H]
\centering
\input{ParseTree.tikz}
\caption{Example of a parse tree}
\end{figure}

%%% ===========================
%\section{Dependency Tree}
%\label{ch:Foundations:sec:DependencyTree}
%%% ===========================

%% ===========================
\section{Reordering Rule Types}
\label{ch:Foundations:sec:types}
%% ===========================



%% ===========================
\section{Lattices}
\label{ch:Foundations:sec:Lattices}
%% ===========================
A word lattice could be presented with a directed acyclic graph. The graph contains nodes and transitions, with each transition labeled with a word. One example is showed in the next page. The word lattice in the example groups different word reorderings of the same English sentence together, with each reordering corresponding a path from the beginning node to the end node. The word lattice provides different p 
pass to decoder,
probability on transitions,
probability of reordering.

\begin{landscape}
\begin{figure}
\centering
\input{Lattices.tikz}
\caption{Example of a word lattice}
\end{figure}
\end{landscape}

\section{BLEU Score}
\label{ch:Foundations:sec:bleu}
BLEU is the de facto standard in machine translation\cite{metrics}. We use BLEU score to evaluate the SMT system throughout this paper.


%% ===========================
\chapter{Reordering Approach}
\label{ch:ReorderingApproach}
%% ===========================

%% ===========================
\section{Reordering Problem in Chinese-English Translation}
\label{ch:ReorderingApproach:sec:Problem}
%% ===========================

%% ===========================
\section{Motivation of our Pre-reordering system}
\label{ch:ReorderingApproach:sec:Motivation}
%% ===========================

%% ===========================
\section{The reordering algorithm}
\label{ch:ReorderingApproach:sec:Algorithm}
%% ===========================