%% evaluation.tex
%%

%% ==================
\chapter{Evaluation}
\label{ch:Evaluation}
%% ==================

We've conducted two sets of experiments to test our reordering methods. The first set of experiments is designed for testing the English-to-Chinese translation, which is described in section~\ref{ch:Evaluation:sec:enw}. The second set of experiments is designed for the other translation direction, which is described in section~\ref{ch:Evaluation:sec:zhen2}. Through the experiments on both translating direction, we could get a better overview of the MLT reordering's effect.

Both sections are composed of three parts: experimental setup, results and evaluation. The first part describes details of the system configurations and experimental data. The second part shows the \hyperref[ch:Foundations:sec:bleu]{BLEU} scores of different systems for comparison. And the last part evaluates the improvement with translation examples from the experiments.


\section{English to Chinese System}
\label{ch:Evaluation:sec:enw}

\subsection{Experimental Setup}
We performed experiments with or without different reordering methods covering the English-to-Chinese translation direction. The reordering methods included the reordering with short rules, long rules, tree rules and the MLT rules. The system was trained on news text from the LDC corpus and subtitles from TED talks. The development data and test data were both news text from the LDC corpus. The system was a phrase-based SMT system, which used a $6$-gram language model with Knersey-Ney smoothing. Besides the preordering, no lexical reordering or other reordering method was used. The text was translated through a monotone decoder. The Chinese text were first segmented into words before use.
The reordering rules were extracted by using the alignment, POS tags and syntax trees from the training data. One reference of the test data was used for evaluating the BLEU scores. The threshold for rule extraction is set as $5$ times and reordering paths with probability less than $0.1$ are not added to the lattice. Table~\ref{denw} shows the size of data used in the system. %? case-sensitive?

\begin{table}[H]
\centering
\begin{tabular}{|ll|r|r|r|r|r|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{Data Set}} & \multirow{2}{*}{Sentence Count} & \multicolumn{2}{c|}{Word Count} & \multicolumn{2}{c|}{Size (Byte)}\\ \cline{4-7}
& & & English & Chinese & English & Chinese \\
\hline
\multirow{2}{*}{Training Data} & \multicolumn{1}{|l|}{LDC} & 303K & 10.96M & 8.56M & 60.88M & 47.27M \\ \cline{2-7}
& \multicolumn{1}{|l|}{TED} & 151K & 2.58M & 2.86M & 14.24M & 15.63K \\ \hline
\multicolumn{2}{|l|}{Development Data} & 919 & 30K & 25K & 164K & 142K \\ \hline
\multicolumn{2}{|l|}{Test Data} & 1663 & 47K & 38K & 263K & 220K \\ \hline
\end{tabular}
\caption{Data details in English-to-Chinese system}
\label{denw}
\end{table}

\subsection{Results}

\begin{table}[H]
\centering
\STautoround*{2}
\begin{spreadtab}{{tabular}{|l|r|r|}}\hline
@				& @BLEU Score & @Improvement \\ \hline
@Baseline		& 12.07 & \\ \hline
@+Short Rules	& 12.50 & :={b3 * 100 /b2 - 100} \% \\ \hline
@+Long Rules   & 12.99 & :={b4 * 100 /b2 - 100} \% \\ \hline
@+Tree Rules   & 13.38 & :={b5 * 100 /b2 - 100} \% \\ \hline
@+MLT Rules    & 13.81 & :={b6 * 100 /b2 - 100} \% \\ \hline
@Oracle Reordering & 18.58 & :={b7 * 100 /b2 - 100} \% \\ \hline
\hline
@Long Rules   & 12.31 & :={b8 * 100 /b2 - 100} \% \\ \hline
@Tree Rules   & 13.30 & :={b9 * 100 /b2 - 100} \% \\ \hline
@MLT Rules    & 13.68 & :={b10 * 100 /b2 - 100} \% \\ \hline
\end{spreadtab}
\caption{BLEU score overview of English-to-Chinese system}
\label{tenw}
\end{table}

Table~\ref{tenw} shows the BLEU scores for configurations with different reordering methods. The table consist of $2$ sections. the first row of the top section shows results of the baseline, which used no reordering at all. In the following rows of the top section, different types of reordering rules are combined gradually, each type per row, and the improvements are showed. For example, the row with \emph{$+$MLT} Rules presents the configuration with all the rule types including MLT rules and all the other rules in the rows above. All the improvements are calculated comparing to the baseline in percentage. Each row with a certain reordering type presents all the different variations of the type and the best score under these configurations are shown. For example, long rules include the left rules and right rules type. In the lower section of the table, different rule types are not combined and the effect of each rule type is shown. %? Besides, the lower section also shows the total number of rules that are extracted and applied for each rule type. (Problem: many variations :(  )

\subsection{Evaluation}

The results shows increasing scores as we used reordering methods from short rules, long rules, tree rules to MLT rules. And better BLEU scores were achieved as we combined the different reordering rules. The MLT rules improved the BLEU score, not only when it was used alone, but also it was added to the other reordering rules. 
%However, by taking a close look at the gap between BLEU scores of oracle reordering and the best BLEU score we've achieved, we can also tell, there's still potential for improvement.

We've also found improvements in the sentence structures. Some translation examples are listed in table~\ref{te}. Each section of this table shows translation of a sentence in its source language and target language. The translation in the rows with \emph{No MLT} comes from the configuration without using MLT reordering, and the translation in the rows with \emph{MLT} comes from the configuration by using MLT reordering. From the examples, we can see that the sentence structure was improved by MLT reordering obviously.

\begin{table}

\centering
\begin{tabular}{|l|m{0.7\textwidth}|} \hline
Source & Hu Jintao also extended deep condolences on the death of the Chinese victims and expressed sincere sympathy to the bereaved families.
\\ \hline
No MLT & \cntext{胡锦涛 还 表示 深切 哀悼 的 受害者 家属 的 死亡 , 向 迂难者 家属 表示 诚挚 的 慰问 。} \\ \hline
MLT & \cntext{胡锦涛 还 对 中国 迂难者 表示 哀悼 , 向 迂难者 家属 表示 诚挚 的 慰问 。} \\ \hline \hline

%Source & The plaintiffs' attorney pointed out that this ruling ``violates the historical record'', and demanded that the Japanese government and Nippon Yakin pay compensations and offer apologies to resolve the issue . \\ \hline
%No MLT & \cntext{这 个 原告 的 律师 指出 , 这 种 `` 违背 了 历史 记录 '' , 要求 日本 政府 , 正 yakin 支付 补偿 , 道歉 来 解决 问题 。} \\ \hline
%MLT & \cntext{原告 的 律师 指出 , 这 一 判决 `` 违背 了 历史 纪录 '' , 要求 日本 政府 , 正 yakin 支付 补偿 , 道歉 来 解决 问题 。} \\ \hline \hline

Source & Satisfying personal interests and expanding knowledge are also major reasons why hourly work appeals to people.\\ \hline
No MLT & \cntext{满足 个人 利益 和 扩大 知识 也 是 主要 原因 小时 工作 吸引 人 。} \\ \hline
MLT & \cntext{满足 个人 利益 和 扩大 知识 也 是 为什么 学生 工作 吸引 人 的 主要 原因 。} \\ \hline \hline

Source & The Dalai Lama will go to visit Washington this month.\\ \hline
No MLT & \cntext{达赖 喇嘛 将 访问 华盛顿 的 这 一 个 月 。} \\ \hline
MLT & \cntext{达赖 喇嘛 将 本 月 访问 华盛顿 。} \\ \hline
%? add more examples?

\end{tabular}
\caption{Examples of translations}
\label{te}
\end{table}
%? change the example, it's wrong
%Each section of table $4.3$ shows translation of a sentence in its source language and target language. The translation in the row with ``No MLT'' comes from the configuration without using MLT reordering, and the translation in the row with ``MLT'' comes from the configuration with using MLT reordering. From the examples, we can see that the MLT reordering improved the sentence structure significantly. In the last example, the part of the sentence ``visit Washington this month'' was parsed as the structur ``(visit (Washington) ((this) (month)))'' in the syntax tree, but the correct Chinese translation has word order corresponds to ``this month visit Washington''. This reordering inserts the word ``visit'' between the words ``this month'' and ``Washington'', which are on a lower level of the syntax tree. This behavior could not be done by the tree-rule-based reordering, which only changes order of constituents on the same tree level.
%
%%picture?

From this experiments we can draw the conclusion that our reordering method indeed improves the English-to-Chinese translation quality obviously, no matter when we apply it alone or when we combine it with other reordering methods mentioned in this paper. %?And we could further justify our claim that the MLT reordering method changes the word order more significantly to improve the translation quality.

%% ===============================
\section{Chinese to English System}
\label{ch:Evaluation:sec:zhen2}
%% ===============================


\subsection{Experimental Setup}

The experiments for Chinese-to-English systems had a similar setup as described in the last section. The parallel data used in the English-to-Chinese system was also used in this experiment by switching the roles of the source language and target language. We only used the LDC data set for training, and no TED data were used in this system. And the test data had three English references for evaluating the results instead of one as in the previous system. The data used are also summarized in table~\ref{dzhen2}.


\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\multirow{2}{*}{Data Set} & \multirow{2}{*}{Sentence Count} & \multicolumn{2}{c|}{Word Count} & \multicolumn{2}{c|}{Size (Byte)}\\ \cline{3-6}
& & Chinese & English & Chinese & English \\
\hline
Training Data & 303K & 8.56M & 10.96M & 47.27M & 60.88M\\ \hline
Development Data & 919 & 25K & 30K & 142K & 164K \\ \hline
Test Data & 1663 & 38K & 47K & 220K & 263K \\ \hline
\end{tabular}
\caption{Data details in Chinese-to-English system}
\label{dzhen2}
\end{table}

\subsection{Results}

\begin{table}[H]
\centering
\STautoround*{2}
\begin{spreadtab}{{tabular}{|l|r|r|}}\hline
@				& @BLEU Score & @Improvement \\ \hline
@Baseline		& 21.80 & \\ \hline
@+Short Rules	& 22.90 & :={b3 * 100 /b2 - 100} \% \\ \hline
@+Long Rules   & 23.13 & :={b4 * 100 /b2 - 100} \% \\ \hline
@+Tree Rules   & 23.84 & :={b5 * 100 /b2 - 100} \% \\ \hline
@+MLT Rules    & 24.14 & :={b6 * 100 /b2 - 100} \% \\ \hline
@Oracle Reordering & 26.80 & :={b7 * 100 /b2 - 100} \% \\ \hline
\hline
@Long Rules   & 22.10 & :={b8 * 100 /b2 - 100} \% \\ \hline
@Tree Rules   & 23.35 & :={b9 * 100 /b2 - 100} \% \\ \hline
@MLT Rules    & 23.96 & :={b10 * 100 /b2 - 100} \% \\ \hline
\end{spreadtab}
\caption{BLEU score overview of Chinese to English systems}
\label{tzhen2}
\end{table}

Table~\ref{tzhen2} shows the BLEU scores for configurations with different reordering methods for the Chinese-to-English translation. The table can be interpreted in the same manner as table~\ref{tenw} in the previous section.

\subsection{Evaluation}

We can tell from table~\ref{tzhen2} that the MLT reorering achieved the best BLEU score under all the rule types, when it was used separately. When the MLT rules was combined with other rule types, it also showed the effect to improve the translation further. Like the results in the previous section, there's also still a large gap between the score we've achieved and the score of oracle reordering, which leaves further possibilities for improvement.%? add manuel analyzing?

From this experiments we can draw the conclusion that our reordering method also improves the translation quality for Chinese-to-English direction.

%? English to German system?

%
%%% ===============================
%\section{Effect dependency tree?}
%\label{ch:Evaluation:sec:e0}
%%% ===============================
%
%
%
%%% ===============================
%\section{Alternative Scanning?}
%\label{ch:Evaluation:sec:a}
%%% ===============================
%
%%% ===============================
%\section{Effect of Different Left Side}
%\label{ch:Evaluation:sec:e1}
%%% ===============================
%
%%% ===============================
%\section{Effect of Different Threshold}
%\label{ch:Evaluation:sec:e2}
%%% ===============================
%
%%% ===============================
%\section{Research on other language pair}
%\label{ch:Evaluation:sec:e3}
%%% ===============================

%
%%==============En to De systems================
%\begin{table}[H]
%\centering
%\STautoround*{2}
%\begin{spreadtab}{{tabular}{|l|r|r|}}\hline
%@				& @BLEU Score & @Improvement \\ \hline
%@Baseline		& 18.45 & \\ \hline
%@+Short Rules	& 19.09 & :={b3 * 100 /b2 - 100} \% \\ \hline
%@+Long Rules   & 19.16 & :={b4 * 100 /b2 - 100} \% \\ \hline
%@+Tree Rules   & 19.34 & :={b5 * 100 /b2 - 100} \% \\ \hline
%@+MLT Rules    & 20 & :={b6 * 100 /b2 - 100} \% \\ \hline
%@Oracle Reordering & 21 & :={b7 * 100 /b2 - 100} \% \\ \hline
%\end{spreadtab}
%\caption{Results of English to German translation}
%\end{table}
%
%\begin{table}[H]
%\centering
%\STautoround*{2}
%\begin{spreadtab}{{tabular}{|l|r|r|}}\hline
%@				& @BLEU Score & @Improvement \\ \hline
%@Baseline		& 18.45 & \\ \hline
%@+Short Rules	& 19.09 & :={b3 * 100 /b2 - 100} \% \\ \hline
%@+Long Rules   & 19.16& :={b4 * 100 /b2 - 100} \% \\ \hline
%@+Tree Rules   & 19.34 & :={b5 * 100 /b2 - 100} \% \\ \hline
%@+MLT Rules    & 1 & :={b6 * 100 /b2 - 100} \% \\ \hline
%@Oracle Reordering & 1 & :={b7 * 100 /b2 - 100} \% \\ \hline
%\end{spreadtab}
%\caption{Results of German to English translation}
%\end{table}
%%==============================


%%% ===============================
%\section{Experiement Result}
%\label{ch:Evaluation:sec:ExperimentResult}
%%% ===============================
%
%%% ===============================
%\section{Evaluation}
%\label{ch:Evaluation:sec:Evaluation}
%%% ===============================

\section{Summary}
\label{ch:Evaluation:sec:Conclusion}

In this chapter, we've presented and evaluated the results of the English-to-Chinese and Chinese-to-English SMT system. In both system, our MLT reordering method shows obvious improvement on the translation quality. The MLT reordering helped to further improve the BLEU score by $3.57\%$, when it was combined with other reordering methods for translating from English to Chinese. And the improvement on the other translating direction was $1.37\%$.

Through analyzing the syntactic structure of the sentences closely, we've also found that the MLT reordering method improved the translation by changing the order of words, not only on the same syntax tree level but also between different levels, which could not be easily achieved by other reordering methods we've introduced so far. So it further justifies our claim, that our MLT reordering method changes the word order more significantly to improve the translation between Chinese and English.
