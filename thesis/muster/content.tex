%% content.tex
%%

%% ===========================
\chapter{Foundations}
\label{ch:Foundations}
%% ===========================

This chapter provides an introduction to fundamental knowledge that are relevant to this work. We start with the whole SMT system and pre-reordering system first, then followed by the information we used to create the reordering rules including alignment, POS tag, syntax tree. At the end we shows the different rule types, the oracle reordering and how to build lattices for translation. \cite{book} also provide a good introduction to statistical machine translation, including different kinds of theories and methods that are relevant to this work.

%% ===========================
\section{SMT System}
\label{ch:Foundations:sec:SMTSystem}
%% ===========================

%Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.\cite{wikismt} One typical archit

Statistical machine translation (SMT) is the state of art machine translation paradigm. It uses a typical log-linear model which is composed of a decoder and different statistical models including phrase table, reordering model and language model. All the models are weighted with parameters which are tuned from the development data. Besides development data, training data are used for training the alignment, phrase table and other models. And test data are used for evaluation purpose. The architecture of a SMT system could be illustrated as figure~\ref{smt}.

\begin{figure}
\centering
\input{smt.tikz}
\caption{Architecture of a SMT system}
\label{smt}
\end{figure}

%% ===========================
\section{Rules Based Pre-Reodering}
\label{ch:Foundations:sec:PreReorderingSystem}
%% ===========================

Our pre-reordering method is based on reordering rules. Reordering rules are rules that tell us how we should reordering the sentences in source language before translating them. In our system, the rules are generated by using the word alignment, part-of-speech (POS) tag and syntax tree, all of which are calculated based on the training data. After we apply the reordering rules to the source sentences, word lattices are generated. The word lattices contains all the reordering possibilities of the source sentences and are further passed to the decoder for translating. The pre-reodering system could be illustrated as figure~\ref{prereordering}.

\begin{figure}
\centering
\input{prereordering.tikz}
\caption{Pre-reordering system}
\label{prereordering}
\end{figure}

A more detailed description of \hyperref[ch:Foundations:sec:Alignment]{word alignment}, \hyperref[ch:Foundations:sec:PosTag]{POS tag}, \hyperref[ch:Foundations:sec:SyntacticTree]{syntax tree},
\hyperref[ch:Foundations:sec:types]{reordering rules} and 
\hyperref[ch:Foundations:sec:Lattices]{word lattices} is also clarified in the following sections.

The \hyperref[ch:ReorderingApproach]{reordering approach} we used for extracting and applying the rules is introduced in the next chapter.

%% ===========================
\section{Word Alignment}
\label{ch:Foundations:sec:Alignment}
%% ===========================

Word alignment indicate the possible alignment between words in the source sentence and words in the target sentence. For example, figure $2.3$ shows an alignment between an English sentence and a Chinese sentence.

\begin{figure}[H]
\centering
\input{alignment.tikz}
\caption{Example of word alignment}
\end{figure}
%? better example with crossing alignment?
Or it may be simply presented as index pairs.
\begin{center}
\verb|7-1 8-1 8-2 9-3 9-4 1-5 1-6 2-6 5-6 3-7 4-8 4-9 10-10|
\end{center}

In the figure, the words that are aligned through lines in two languages have related meaning. We can see how the words are reordered from the figure. For exmaple, the noun clause ``Clinton's presidency'' is completely moved forward to the front of the sentence.

\phantomsection\label{alignedrange}
It is noteworthy that the alignment is not always a one-to-one match. In the example, the word ``of'' is not aligned at all and the word ``months'' is aligned to two word ``\cntext{个}'' and ``\cntext{月}'' in Chinese. We can define the aligned range as the range from the first word a certain word is aligned to to the last word it is aligned to. For example, the aligned range of word ``there'' is $[5,6]$. The \textbf{aligned range} can be coincided, such as the $6$th word ``\cntext{剩下}'' in Chinese is also aligned to ``are'' and ``left'' at the same time besides ``there''. The coincidence sometimes makes the detection of rules more difficult, because the word order is not clear any more.

The word alignment could be trained with the GIZA++ tool by using Expectation Maximization (EM) algorithm. From the word alignment of training data we can see the patterns how the words are reordered before and after translation. Therefore, we could extract these reordering rules and apply them on the text, which is to be translated.

%% ===========================
\section{Part-of-Speech Tag}

Part-of-speech (POS) tags are markups of words in the text, which indicates the syntactic role of part of the speech. The markups are based on words' definitions and their context. Figure~\ref{tags} shows a sentence with the POS tags. Table~\ref{ttags} lists part of the Penn Treebank tagset for quick reference. A complete list can be viewed in appendix~\ref{tagset}.

\begin{figure}[H]

\centering
\input{tags.tikz}
\caption{Example of POS tags}
\label{tags}
\end{figure}

\begin{table}
\centering
\begin{tabular}{|p{4.5cm}l|}
\hline
$1$. \hphantom{1}CC &  Coordinating conjunction\\
$2$.  \hphantom{1}CD &  Cardinal number\\
$3$. \hphantom{1}DT &  Determiner\\
$4$. \hphantom{1}IN &  Preposition/subordinating conjunction\\
$5$. \hphantom{1}JJ &  Adjective\\
$6$. \hphantom{1}JJR &   Adjective, comparative\\
$7$. \hphantom{1}JJS &   Adjective, superlative\\
$8$. \hphantom{1} MD &  Modal verb\\
$9$. \hphantom{1} NN &  Noun, singular or mass\\
$10$. NNS &   Noun, plural\\
$11$. RB &  Adverb\\
$12$. VB &  Verb, base form\\
$13$. VBP &   Verb, non-3rd person singular present\\
$14$. VBZ &   Verb, 3rd person singular present\\
$15$. WRB &   \textit{wh}-adverb\\
$16$. . &   Sentence-final punctuation\\
$17$. ADJP &  Adjective phrase\\
$18$. ADVP &  Adverb phrase\\
$19$. NP &  Noun phrase\\
$20$. PP &  Prepositional phrase\\
$21$. QP &  Quantity phrase\\
$22$. S &  Simple declarative clause\\
$23$. VP &  Verb phrase \\ \hline
\end{tabular}
\caption{Penn Treebank tagset}
\label{ttags}
\end{table}
\label{ch:Foundations:sec:PosTag}
%% ===========================

%% ===========================
\section{Syntax tree}
\label{ch:Foundations:sec:SyntacticTree}
%% ===========================

The syntax tree shows the syntactic structure of a sentence and can be very useful for word reordering. A syntax tree contains two kinds of nodes: the leaves and the internal nodes. Each leaf presents a word in the sentence, and is annotated with the POS tag. And each internal node presents a constituents, which is also annotated to indicate its category or syntactic role. In the Penn treebank \citep{penn, penn3}, for example, the annotation \emph{NP} means noun phrase and the annotation \emph{S} means simple declarative clause. Figure~\ref{ParseTree} is an example of a syntax tree.

\begin{figure}
\centering
\input{ParseTree.tikz}
\caption{Example of a parse tree}
\label{ParseTree}
\end{figure}

We can see the syntactic structure of the sentence from the syntax tree very clear. In this example, The words ``math and biology exams'' make up a noun clause, which plays the roll of subject. The predicate has a nested structure of verb clause, because it's compound with the modal verb ``will'' and the word ``be''. And ``on the 27th'' is a preposition clause in the verb clause, which is again composed of a preposition ``in'' and a noun clause ``the 27th''.

%%% ===========================
%\section{Dependency Tree}
%\label{ch:Foundations:sec:DependencyTree}
%%% ===========================

%% ===========================
\section{Reordering Rules}
\label{ch:Foundations:sec:types}
%% ===========================
%Short rules was introduced by \cite{short}.
%\subsection{Short Rules}
%\subsection{Long Rules}
%\subsection{Tree Rules}

Based on \cite{short}, \cite{long}, \cite{tree} and \cite{combine}, we introduce the different rule types, rule combination, how to decide if rules can be extracted as well as how to calculate the probability to apply the rules.

\subsection{Short Rules}

Short rules are extracted based on the sequences of adjacent words or their POS tags in the sentence to translate. Sequences of adjacent words or tags are observed, rules are then extracted if the same reordering patterns appear frequently. Following are some examples:
$$\verb|after the accident -> the accident after (0.5)|$$
$$\verb|WRB MD DT -> DT WRB DT (0.3)|$$
The first rule in this example shows, if a the word sequence \emph{after the accident} ever appears in the text, it should be reordered to \emph{the accident after} with a probability $0.5$, so will the word order be more consistent with the translation. The second rule shows, if the word sequence of a \emph{wh}-adverb (\emph{when}, \emph{where}, \emph{why}, etc.), a modal verb (\emph{MD}) and a determiner (\emph{DT}) appears, the determiner should be moved before the \emph{wh}-adverb with a probablity of $0.3$.

In addition, Short rules also have some different varieties \citep{short}:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Tag sequence:} rules are extracted based on adjacent tag sequence
\item \textbf{Word sequence:} rules are extracted based on adjacent word sequence
\item \textbf{Context of one or two tags before and/or after the tag sequence}
\item \textbf{Context of one or two words before and/or after the tag sequence}
\end{itemize}

\subsection{Long Rules}

Long rules are specially designed to help the long distance word reordering for translation between English and German. The rules are based on POS tags of the text, and an example is as following:
$$\verb|NN X MD VBP -> X MD NN VBP (0.14)|$$

The ``X'' in the example is an placeholder, which can replace one or more words. ``VHP'' is the right context, which is sometimes helpful to define the reordering boundary. ``NN'' means noun, ``MD'' means modal verb and ``VHP'' is the word ``have''. In this example, the tag sequence ``NN X MD VHP'' will be reordered as ``X MD NN VHP'' with a likelihood of $0.1429$.

Rules are extracted by first finding the location of the reordering rule and then putting the placeholder. Depends on the location, where the placeholder is put, and how much the placeholder replace,  the long rules also have some varieties:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Left/right rules:} depends on if the placeholder is put on the left part or right part
\item \textbf{All/part replacement:} depends on if all the words in a part is replaced by the placeholder
\end{itemize}

\subsection{Tree Rules}
\label{treerules}

While short rules and long rules are based on the flat structure of the sentence, tree rules reorders the sentence by using information from the sentence's syntactic structure. The syntax tree and alignment of the training corpus are used to train the rules. The tree rules reorder the words both on the word level and on the constituent level. Following is an example:
$$\verb|NP ( ADJP JJ NN ) -> JJ NN ADJP (0.06)|$$
The parenthesis in the example represents the hierarchies in the syntax tree. The left side of the rule corresponds a tree with root labeled with ``NP'' and three children, each labeled with ``JJ'', ``NN'' and ``ADJP''. When this structure appears as a subtree in the syntax tree of the sentence to translate, the order of its subtrees should be changed. The change is illustrated in figure $f$.

\begin{figure}[H]
\centering
\input{swap.tikz}
\caption{Change subtree order based on tree rules}
\end{figure}

Tree rules also have some different varieties:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Partial Rules:} the relatively flat syntactic structure of languages like German may make the rule extraction difficult, because the extraction requires that the whole subtree including all its children is matched. In order to extract more useful information for reordering, rules are also extracted from any partial child sequence in a constituent.
\item \textbf{Recursive rule application:} the rules may be applied recursively to already reordered sentence. And all paths of the reorderings are added to the lattice.
\end{itemize}


\subsection{Rule Extraction and Application}
\label{general}

Rules are extracted by scanning all the training data and detecting the word order change. A valid word order change that can count for reordering rule needs to fulfill the two criteria in general:
\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Order change exists:} otherwise, there's no need for reordering rules.
\item \textbf{No \hyperref[alignedrange]{aligned ranges} coincidence:} the coincidence makes it hard to decide the new word orders in target language.
\end{itemize}

Rules are not always extracted upon discovering of word change. In order to avoid too excessively concrete rules which don't apply well in general, we extract reordering rules only when the same reordering pattern appears more than a certain threshold. Thus it won't lead to overfitting.

The associated probabilities of reordering rules is the frequency how often the sequence in the rules are reordered in the same manner. For example, if the sequence ``after the accident'' appears many times in the training data, and half of the time, it's reordered as ``the accident after'', then the probability of the reordering rule is calculated as $50\%$.

Rules are applied by scanning the text to be translated. When there's a sequence coincides the left side of the reordering rules, rules will be applied, and a path in the \hyperref[ch:Foundations:sec:Lattices]{word lattice} representing the reordered word will be added.

\subsection{Rule Combination}

In order to further explore the probability of improvement, we combine different rules for reordering in our experiment. This is done by training the different types of rules separately and applying them on the monotone path of the sentence independently. They result different paths in the lattice.



%% ===========================
\section{Oracle Reordering}
\label{ch:Foundations:sec:oracle}
%% ===========================

In order to evaluate the potential of word reordering, we introduce the oracle reordering. Oracle reordering is considered to be an optimally reordered sentence as input to the SMT system and do not allow additional reordering during decoding. \citep{combine} The oracle reordering is created by using the permutation of source sentences, which is extracted from the word alignment between the source text and reference.

In order to abstract the permutation from the word alignment, some cases need to be considered, since word alignment is generally not a one-to-one word mapping. There're the following four cases: \citep{birch2}

\begin{itemize}
\setlength{\itemsep}{0cm}%
\setlength{\parskip}{0cm}%
\item \textbf{Unaligned source words:} are assigned to the target word position immediately after the target word position of the previous source word, or to position $1$ if they're at the beginning of the source sentences
\item \textbf{Unaligned target words:} are ignored
\item \textbf{Many-to-one alignment:} the target ordering is assumed to be monotone
\item \textbf{One-to-many alignment:} the source word is assumed to be aligned to the first target word
\end{itemize}

Because it's considered as an optimally reordering, we can use it as input of the SMT system and the resulted BLEU score represents optimal score that can be achieved by word reordering, from which we can evaluate the potential of reordering methods.

%% ===========================
\section{Word Lattice}
\label{ch:Foundations:sec:Lattices}
\label{latticecreation}
%% ===========================
A word lattice could be presented with a directed acyclic graph. The graph contains nodes and transitions, with each transition labeled with a word and a probability. The outgoing transitions from a node indicate different options, which words could come after this point. The annotation on the transition indicate the word that could come, together with the probability of this option. The word lattice groups different word reorderings of the same English sentence together, with each reordering corresponding a path from the beginning node to the end node. 

Word lattice for reordered word sequences is build gradually while applying the reordering rules on the sentence. It starts with a monotone path presenting the sentence to translated. Every time when a rule is applied and part of the sentence is reordered. We add a parallel path to the corresponding part of the initial monotone path. The parallel path is labeled with reordered words on its transitions. The probability of this new reordering is subtracted from the first transition after the splitting point on monotone path, and assigned to the first transition of the new path. All the other transitions on the new path that follow have a probability of $1$.

%? graph for building?

Paths with very low probability are removed, in order to save space for storing the lattice and reduce decoding time later, without compromising to much translation quality. %?In our system, we remove all the paths that have a probability lower than $0.1$.

An example of a word lattice is showed in figure $2.6$. If the probability of a transition is $1$, it's left out in the example to keep the graph clear.


\begin{landscape}
\begin{figure}
\centering
\input{Lattices.tikz}
\caption{Example of a word lattice}
\end{figure}
\end{landscape}

\section{BLEU Score}
\label{ch:Foundations:sec:bleu}

BLEU (Bilingual Evaluation Understudy) is an algorithm to evaluate the translation quality of machine-translated text. It shows a high correlation with human judgments of translation quality \citep{bleuscore}, and remains one of the most popular metrics in statistical machine translation.

As described in \cite{metrics}: BLEU is the de facto standard in machine translation. It captures the n-gram precision of the translation. Shorter $n$-gram precision captures the lexical coverage of the translation and word order is evaluated by the higher order $n$-grams. The final score is an interpolation of these precisions and it is adjusted by a brevity penalty.

BLUE score is always a number between $0$ and $1$. This value measures how close the translation and reference are, with $1$ indicting the both are identical. In our experiments, BLEU score was used as the measurement for translation quality of SMT system.
\section{Summary}

In this chapter, we've introduced some fundamental knowledge that are relevant to this work, which including the architecture of a SMT system, the pre-reordering system, word alignment, POS tag, syntax tree, different types of reordering rules, oracle reordering, word lattice and BLUE score. For the reordering rules, we've introduced three different types: short rules, long rules and tree rules. In the following chapters, we'll introduce our MLT reordering rules and compare these these different reordering rules.

%% ===========================
\chapter{Reordering Approach}
\label{ch:ReorderingApproach}
%% ===========================

%% ===========================
\section{Reordering Problem in Chinese Translation}
\label{ch:ReorderingApproach:sec:Problem}
%% ===========================

English and Chinese belong to different language groups. As Chinese belong to the Sino-Tibet language group, while English belong to the Indo-Germanic language group. And they have also developed separately for a long time. Because of their different origins and development, they've becoming two very different languages.

Unlike the most languages in the Indo-Germanic language group, which are more close to English than Chinese, Chinese has some properties those languages don't have, such as Characters as fundamental element instead of letters, the tones, no word separation, the usage of measure words,much more simple inflections and conjugations, which raises further problem for machine translation.

Even the word order between English and Chinese differs more significantly. For one, the words in Chinese have generally different origins as those in English, which leads to very different vocabulary and word construction. Sometimes it's very hard to find corresponding words in the other language. For example, some prepositions in Chinese have very different usage than those prepositions in English. Also the continuous writing of Chinese without space makes this problem more severe, since word boundaries are not always so clear in Chinese. So text needs to be segmented first before translation. A word segmentation process is conducted to separate the word, but the result may not always be ideal.

For the other, both languages have sometimes very different sentence structure. Thus, a word-for-word translation between the English and Chinese is often unnatural or difficult to understand. Each of them has some sentence patterns that don't exist or rarely used in the other. In Chinese, a part of sentence that modifies another part may tend to be put before that part that it modifies. While in English, it's very common that the part that modifies others may be put after them. Besides, an English sentence with a lot of long clauses may be more suitable to translate into several Chinese sentences, because Chinese doesn't tend to use long clauses. 

Some literature \citep{syntactic} has discussed or analyzed the differences in word orders between English and Chinese. Through analyzing the data we have and study of the literature, we've found several major types of differences in word orders between English and Chinese, which are typical in the data and often lead to translation problems. They are listed as follows.

\paragraph{Relative clauses}
Typically a relative clause modifies a noun or noun phrase. In English a relative clause is normally put after the noun or noun phrase that it modifies. While in Chinese, it's normally put before the noun or noun phrase. But sometimes, a relative clause may also be separated to from another sentence if it is too long. This makes the sentence look more balance. Following is an example to show the position change of a relative clause.
\begin{figure}[H]
\centering
\input{relative.tikz}
\caption{Position change of an adverbial of time}
\end{figure}

\paragraph{Adverbials}
An adverbial can be an adverb, an adverbial phrase or an adverbial clause that modifies the verb or the whole sentence. The position of adverbials is a complicated topic. In general, the location of adverbials in a sentence are very flexible. They can be located in different position of a sentence both in English and Chinese, such as beginning, middle or end of a sentence, before or after the verb. The location varies and it often depends on the situation. When comparing English and Chinese word order, the location of adverbials in one language doesn't automatically implies the same location in the other. Typical examples are adverbials of time, location and frequency. It's often put after the verb in English, but before the verb in Chinese. 
\begin{figure}[H]
\centering
\input{adverbial.tikz}
\caption{Position change of an adverbial of time}
\end{figure}

\paragraph{Preposition phrases}
A preposition phrase functions sometimes as an adverbial, and it should be reordered before the verb for translation. But a preposition phrase can also modify a noun or noun phrase sometimes. When a preposition phrase modifies a noun or noun phrase, it's typically located after the noun or noun phrase that it modifies. While in Chinese it's typically located before the noun or noun phrase that it modifies.

\begin{figure}[H]
\centering
\input{pp.tikz}
\caption{Position change of an adverbial of time}
\end{figure}

\paragraph{Questions}
Questions are often formed by moving the auxiliary verb before the subject or adding \textit{do} (\textit{does}, \textit{did}) before the subject if there's no auxiliary verb in English. And interrogative word such as \textit{where}, \textit{what}, \textit{how}, etc. is added if it's an interrogative question. However, building a question in Chinese doesn't affect the sentence structure so much. Generally, the questioned part is replaced with a interrogative word and a question denominator is put at the end of sentence to indicate the question. In the Chinese question in figure $3.4$, the verb stays after the subject, the interrogative word is put at the location where an adverbial of manner is generally put, and an question denominator is put before the question mark.

\begin{figure}[H]
\centering
\input{question.tikz}
\caption{Position change of an adverbial of time}
\end{figure}

\paragraph{Special Sentence Constructions}
Due to the lack of certain sentence patterns, the reordering could be untypical or it varies from case to case. Chinese doesn't have sentence construction corresponding to the inverted negative sentences or \textit{there be} sentences in English. Meanwhile, the Chinese \linebreak\textit{bǎ}-construction (\cntext{把字句}) doesn't exist in English either.

\begin{figure}[H]
\centering
\input{therebe.tikz}
\caption{Position change of an adverbial of time}
\end{figure}

%?correct compound subjects: or & there be...
%% ===========================
\section{Motivation of Reordering on Multiple Syntactic Levels}
\label{ch:ReorderingApproach:sec:Motivation}
%% ===========================

Because English and Chinese have different word orders and there're also some special cases of sentence patterns, the word reordering can be complicated and unsystematically. From the differences in word orders that we've discussed in last section, we can see two kinds of word position change are very typical.

\subsection{Long-Distance Word Reordering}

Because sentence structure is often changed dramatically when translating between English and Chinese, the reordering often involves with long-distance word position change. Not just the position change of a part of sentence may be a long-distance change, the part that is moved may also be very long. For example, an adverbial clause of time may located at the end of a English sentence, but in order to be processed for translation into Chinese, it may need to be moved across the whole sentence to the front, and the adverbial clause itself may be also very long. 

\paragraph{Example $1$}
I find this very much disturbing \textit{\ul{when we are talking about what is going on right and wrong with democracy these days}}.\smallskip\\
\cntext{\uline{现在，每当我跟别人讨论我们的民主什么是对的，什么是错的}我都为此觉得很无力。}

\paragraph{Example $2$}
You feel intense elation \textit{\ul{when things are going well}}; mood swings into horrible despair \textit{\ul{when things are going poorly}}.\smallskip\\
\cntext{\uline{当事情进展顺利的时候，}你会觉得兴高采烈；\uline{当事情不顺利的时候，}你又会陷入极度的失望和恐慌。}

In both exapmles the adverbial clauses are moved to the front. These are common cases in translation between English and Chinese. In order to be able to handle the reordering between English and Chinese, the reordering approach should allow some words to be shifted across a long distance.

\subsection{Word Reordering on Multiple Syntactic Levels}

Sometimes the reordering involves word position changes on multiple syntactic levels. We can see this problem from the examples in figure $3.1$. In the figure, the syntactic structure and alignment of the parallel text are presented in a intuitive way.

\begin{figure}[H]
\centering
\subfigure {
\input{unstructured.tikz}
}
\subfigure {
\input{unstructured2.tikz}
}
\caption{Examples of reordering on multiple syntactic levels}
\end{figure}

From the examples we can also clearly see how to reordering on multiple syntactic levels works. From the root of the syntax tree on the left, if we inspect 3 syntactic levels downwards, we can find the following pattern of word position change, comparing the parallel Chinese text:
$$\texttt{NP ( \ul{CD} NP ( NP ( \ul{JJ NNS} ) PP ( \ul{IN NP} ) ) ) -> NP IN CD JJ NNS}$$
As we can see, we can't get this pattern if we only observing subtrees on the same level of syntax tree. The node labeled with ``CD ten'' is inserted into the subtree of its sibling labeled with ``NP'', between its sibling's two children. This position change can not simply be done by swapping children of the same node.

We can also observe the same phenomenon from the second example. If we inspect two levels downwards from the root node, we can find the following pattern:
$$\texttt{S ( NP ( \ul{NP PP} ) VP ( \ul{VBP ADVP}) \ul{.} ) -> ADVP NP PP VBP .}$$
In this example, the adverbial phrase ``annually'' is moved forward to the front of the sentence, leaving the subtree ``NP ( NP PP )'', which is on a higher level, between it and its sibling.

Besides, there are several reasons why we need word reordering on multiple syntactic levels. First, the syntactic parser may make mistakes. As we found in the training corpus, it's not rare that sentences are misparsed, either the words are not correctly tagged, or the syntactic structure is wrong. Second, the syntax tree of the English sentence may not be suitable for translation into Chinese. In these case, MLT reordering could be used as a remedy for incorrect parsing patterns in the syntax tree. Besides due to the very different word orders between English and Chinese, simply reordering the words by change nodes on the same syntax tree level may not do the job, and MLT reordering will be useful in this case.

In conclusion of the existing reordering rules we've introduced and the problems we've seen by translating between English and Chinese, a good approach for the reordering should both take long-distance reordering and reordering on multiple syntactic levels into account. A long rule based reordering may not utilize the syntactic information for reordering, so the structure of Chinese sentence may not be reconstructed. On the other side, a tree rule based reordering may not be enough helpful by too complicated structure changes, as we've found out there're cases that a reordering can not simply done by swapping children of subtrees. 

Inspired by the method of tree rules based reordering method, we created this reordering algorithm. The algorithm solely uses information of the syntax tree and alignment of the source side. It further explores the syntactic structure of text to be translated, and detecting reordering patterns from multiple levels of the syntax tree altogether.

%% ===========================
\section{The SMT Reordering Algorithm}
\label{ch:ReorderingApproach:sec:Algorithm}
%% ===========================

As we've ready seen how the basic idea of finding reordering patterns on multiple syntactic levels generally works in the last section. We'll systematically explain the rule extraction and application in all details in this chapter.

\subsection{Rule Extraction}

In order to find as much as information for reordering as possible. The algorithm of rule extraction detects the reordering pattern from all nodes in the syntax tree, goes downwards for any number of hierarchies, until it reaches the last hierarchy in the subtrees.

In the implementation, the program conducts a depth-first search (DFS) to traverse every node in the syntax tree. Every time when a node is reached, the program conducts another iterative deepening depth-first search (IDDFS) in its subtree with depth-limit from $1$ to the subtree's depth. And the program detects if there're any patterns of word position changes at the same time, by using the alignment for comparison.

The detected word position changes are checked for their validity for reordering rules. As describe in \hyperref[general]{section $2.6.4$}, a valid patterns for reordering should both be involved with actual reordered words and have clearly distinguished new order from the target side, i.e. no coincidence of aligned range on the target side.

\begin{figure}[H]
\centering
\input{extract.tikz}
\caption{Illustration of rule extraction}
\end{figure}

Figure $3.2$ shows a phrase to be translated together with is syntax tree and alignment to its parallel text in Chinese. The nodes are labels with numbers. In this example, we can find the following reordering patterns:

From node $1$:\\
\texttt{NP ( \ul{NP} \ul{PP} ) -> PP NP} \hfill [1 level]\\
\texttt{NP ( NP ( \ul{JJ} \ul{NNS} ) PP ( \ul{IN} \ul{NP} ) ) -> NP IN JJ NNS} \hfill $*$[2 levels]\\
\texttt{NP ( NP ( \ul{JJ} \ul{NNS} ) PP ( \ul{IN} NP ( \ul{JJ} \ul{NNS} ) ) ) -> JJ NNS IN JJ NNS} \hfill [3 levels]

From node $3$:\\
\texttt{PP ( \ul{IN} \ul{NP} ) -> NP IN} \hfill [1 level]\\
\texttt{PP ( \ul{IN} NP ( \ul{JJ} \ul{NNS} ) ) -> JJ NNS IN} \hfill $*$[2 levels]

It's noteworthy that our method can detect more reordering patterns than the tree rule based method. For example, the patterns marked with $*$ above can not be extracted with the tree rules based method directly.

The probability of the reordering patterns can be calculated as described in \hyperref[general]{section $2.6.4$}. There are the left part and the right part of the reordering patterns separated by the arrow. The left part indicates the sequence that should be reordered and the right part indicates how the new order should be like. The probability of the pattern is calculated by how often the left part is reordered into the right part among all its appearance in the training corpus. In addition, reordering patterns that appear less than a threshold are ignored to be used as reordering rule, in order to prevent too concrete rules without generalization capability and overfitting.

\subsection{Rule Application}

The syntax tree is traverse by DFS as the same in rule extraction. But from the root of each subtree, it's scanned with depth limit from its maximal levels, i.e. its depth, to $1$. If it turns out, any rule can be applied for a subtree at some level, a new path for this reordering will be added to the word lattice for decoding, as introduced in \hyperref[latticecreation]{section $2.8.1$}. As long as rules can be applied on a subtree for a certain depth, the rules are applied and the search for rule application on this subtree stops, and the search on the next subtree continues. 

The reason for this is to prevent duplicate reorderings due to application of rules, which has overlapped effect with each other. These rules are normally patterns that are generated on the same tree, but with different number of levels, which has different generalization effect on the same range of words in the text. For exmaple, the following patterns can be detected from the syntax tree in figure $3.2$ in last section:
\begin{center}
\begin{tabular}{l}
\texttt{PP ( \ul{IN} \ul{NP} ) -> NP IN}\\
\texttt{PP ( \ul{IN} NP ( \ul{JJ} \ul{NNS} ) ) -> JJ NNS IN}
\end{tabular}
\end{center}
Both patterns are detected from the same node, but the second pattern is detected by retrieving the nodes one level deeper and it's more concrete. So the first pattern can be seen as a generalization of the second pattern. Whenever a rule of the second pattern can be applied, a rule of the first pattern can be applied too. Because subtrees are checked from the highest number of levels by rule application, the more concrete rule is applied first. Because the more concrete rule fits the detected pattern better and contains more details of reordering, so it may be more suitable for rule application. In this example, the second rule is applied rather than the first rule.

To illustrate how the rule application work in a more intuitive way, we present an example. Assume we have several reordering rules and a pre-processed sentence for reordering as follows:
\begin{center}
\begin{tabular}{l}
Rules:\\
\text{[1]} \texttt{VP ( \ul{VBZ} NP ( \ul{NP} \ul{PP} ) ) -> PP VBZ NP (0.18)}\\
\text{[2]} \texttt{NP ( NP ( \ul{NN-1} \ul{NN-2} ) PP ( \ul{IN} \ul{NP} ) ) -> NP IN NN-1 NN-2 (0.17)}\\
\\
Sentence:\\
\texttt{world bank plans debt relief for poorest countries}
\end{tabular}
\end{center}

The syntax tree and monotone path as initial word lattice looks as follows:
\begin{figure}[H]
\centering
\subfigure{
\input{left1.tikz}
}
\subfigure{
\input{right1.tikz}
}
\caption{Illustration of rule application (1)}
\end{figure}

By using DFS to traverse the syntax tree, the program first finds out the pattern started from node ``VP'' with $2$ levels corresponds the left part of the first rule listed above. This indicates the rule is applicable at this position. According to the reordering rule, the order of the three constituents labeled with ``VBZ'', ``NP'' and ``PP'' should be changed to ``PP VBZ NP'' with probability $0.18$. Thus the words are reordered into ``for poorest countries plans debt relief'', and the new path with this probability is added to the word lattice.

\begin{figure}[H]
\centering
\subfigure{
\input{left2.tikz}
}
\subfigure{
\input{right2.tikz}
}
\caption{Illustration of rule application (2)}
\end{figure}

As the program keeps going, it turns out the second rule is applicable at another node labeled with ``NP'' with $2$ levels. Again the rule is applied with probability $0.17$ and the new path is added. It should be noted that there're numbers added to the two ``NN'' tags in the rule, in order to distinguish them for the reordering. The program uses index for the internal presentation of words, which doesn't cause any confusion.

\begin{figure}[H]
\centering
\subfigure{
\input{left3.tikz}
}
\subfigure{
\input{right3.tikz}
}
\caption{Illustration of rule application (3)}
\end{figure}

\section{Summary}
In this chapter, we've present the differences in word orders between English and Chinese and introduced the SMT reordering algorithm. Because of the different origins and separate development of the two languages, they have very distinct sentence structures. To adjust the word order in one language to the other as pre-reordering for translation between the two. It often involved with long-distance position change and reordering on multiple level of the syntactic structure. Inspired by the method of tree rules based reordering method, we created the SMT reordering algorithm to rearrange the words as a pre-process for translation. The algorithm uses the information of syntactic tree and word alignment, it extract and applying the reordering rules by detecting patterns from the subtrees with different search levels in the syntax tree.
